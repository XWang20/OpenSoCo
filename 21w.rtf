{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fnil\fcharset0 HelveticaNeue-Medium;\f2\fnil\fcharset0 HelveticaNeue-Bold;
}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red1\green16\blue31;\red0\green0\blue0;
\red255\green255\blue255;\red1\green24\blue49;\red24\green121\blue255;\red0\green0\blue0;\red70\green188\blue21;
\red224\green245\blue255;\red245\green245\blue245;\red249\green249\blue249;\red0\green0\blue0;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000\c65098;\cssrgb\c0\c8235\c16078;\cssrgb\c0\c0\c0\c65098;
\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c12941\c25098;\cssrgb\c9412\c56471\c100000;\cssrgb\c0\c0\c0\c85098;\cssrgb\c32157\c76863\c10196;
\cssrgb\c90196\c96863\c100000;\cssrgb\c96863\c96863\c96863;\cssrgb\c98039\c98039\c98039;\cssrgb\c0\c0\c0\c25098;\cssrgb\c0\c0\c0\c45098;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid1\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid101\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid201\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f0\fs36 \cf2 \cb3 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/"}}{\fldrslt \expnd0\expndtw0\kerning0
Jeeves - The Platform of Platforms}}
\fs28 \cb1 \expnd0\expndtw0\kerning0
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf4 \cb5 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
wx8993 - AI \uc0\u35757 \u32451 \cf2 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf4 \cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/notebooks"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u31508 \u35760 \u26412 }}\cb1 \expnd0\expndtw0\kerning0
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf4 \cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/projects"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u35757 \u32451 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/modelsGroup"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u27169 \u22411 \u32452 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/models"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u27169 \u22411 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/services"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u26381 \u21153 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/admin"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u31649 \u29702 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/list"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u25991 \u20214 \u31649 \u29702 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/database"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u25968 \u25454 \u38598 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/data_access_application"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u25968 \u25454 \u35775 \u38382 \u30003 \u35831 }}\cb1 \expnd0\expndtw0\kerning0
\
\ls2\ilvl0\cb5 \kerning1\expnd0\expndtw0 		{\field{\*\fldinst{HYPERLINK "https://wiki.in.zhihu.com/pages/viewpage.action?pageId=340009276"}}{\fldrslt \expnd0\expndtw0\kerning0
\uc0\u25991 \u26723 }}\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc\partightenfactor0
\cf5 \cb6 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf4 \cb5 \uc0\u20219 \u21153  - jeeves-agi/{\field{\*\fldinst{HYPERLINK "https://jeeves.in.zhihu.com/projects/5401"}}{\fldrslt \cf7 wx-pretrain-scatch}}\cf8 \cb1 \
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf4 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf4 \
\pard\pardeftab720\sa80\partightenfactor0

\f1\fs28 \cf4 \cb5 2023-07-16T22:34:07+08:00\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f0 \cf4 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf9 \cb5 \uc0\u8232 
\f2\b \cf4 \uc0\u36816 \u34892 \u29615 \u22659 
\f0\b0\fs24 \cf7 \cb10 klara-2-pek02custom::pytorch1.12.1-bmtarin0.2.1cfs-shared
\fs28 \cf4 \cb1 \uc0\u8232 \cb5 a100\uc0\u65306 
\f2\b 64
\f0\b0 CPU\uc0\u65306 
\f2\b 512
\f0\b0 Memory\uc0\u65306 
\f2\b 1024Gi
\f0\b0 \cb1 \uc0\u8232 \cb5 Owner\uc0\u65306 
\f2\b wx8993
\f0\b0 \cb1 \uc0\u8232 \cb5 \uc0\u35757 \u32451 \u25968 \u25454 \u65306 \cb1 \uc0\u8232 \cb5 Git \uc0\u20179 \u24211 \u65306 
\f2\b git@git.in.zhihu.com:wx8993/OpenSoCo.git
\f0\b0 \cb1 \uc0\u8232 \cb5 Git \uc0\u20998 \u25903 \u65306 
\f2\b jeeves
\f0\b0 \cb1 \uc0\u8232 \cb5 Commit id\uc0\u65306 {\field{\*\fldinst{HYPERLINK "https://git.in.zhihu.com/wx8993/OpenSoCo/-/commit/3209fb80547d605db9acf4f6252de83f44646267"}}{\fldrslt \cf7 3209fb80}}\cb1 \uc0\u8232 \cb5 \uc0\u26631 \u31614 \u65306 sft\cb1 \uc0\u8232 \cb5 \uc0\u21551 \u21160 \u21629 \u20196 \u65306 
\f2\b bash deberta_jeeves.sh
\f0\b0 \cb1 \uc0\u8232 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 \cf4 \uc0\u24320 \u22987 \u26102 \u38388 \u65306 2023-07-16T22:34:07+08:00\cb1 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 \cf4 \uc0\u24403 \u21069 \u29366 \u24577 \u65306 Killed\cb1 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 \cf4 \uc0\u32467 \u26463 \u26102 \u38388 \u65306 2023-07-17T10:43:19+08:00\cb1 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 \cf4 \uc0\u27169 \u22411 \u20301 \u32622 \u65306 \cb1 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 {\field{\*\fldinst{HYPERLINK "https://jeeves-apps.in.zhihu.com/klara-2-pek02/jeeves-agi/pytorchjob-wx-pretrain-scatch-390755/tensorboard/"}}{\fldrslt \uc0\u26597 \u30475  Tensorboard}}\cf4 \cb1 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 
\fs24 \cf8 \cb1 \dn3 \uc0\u8232 
\fs28 \cb11 \up0 \uc0\u26597 \u30475 \u25351 \u26631 \cb1 \uc0\u8232 \cf4 \cb11 \uc0\u8232 \cb12 \uc0\u8232 \cb1 \uc0\u8232 \
\ls3\ilvl0\kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 \cf7 \cb5 \uc0\u8232 {\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-master-0%22"}}{\fldrslt master-0}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-0%22"}}{\fldrslt worker-0}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-1%22"}}{\fldrslt worker-1}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-2%22"}}{\fldrslt worker-2}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-3%22"}}{\fldrslt worker-3}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-4%22"}}{\fldrslt worker-4}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-5%22"}}{\fldrslt worker-5}}{\field{\*\fldinst{HYPERLINK "https://logging.in.zhihu.com/discover?interval=&kql=app_name:%22jeeves-agi%22%20AND%20unit_name:%22wx-pretrain-scatch-5401%22%20AND%20version:%22390755%22%20and%20pod:%22pytorchjob-wx-pretrain-scatch-390755-worker-6%22"}}{\fldrslt worker-6}}\cf4 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\qc\partightenfactor0
\ls3\ilvl0\cf7 \cb5 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u8232 
\fs24 \cf8 \cb1 \dn3 \uc0\u8232 
\fs28 \cb11 \up0 console \uc0\u26085 \u24535 \cb1 \uc0\u8232 
\f2\b \cf13 \uc0\u8232 
\f0\b0 \uc0\u8232 
\f2\b \cf14 \uc0\u8232 
\f0\b0 \uc0\u8232 
\f1 \cf7 \cb11 logs
\f0 \cf4 master-0worker-0worker-1worker-2worker-3worker-4worker-5worker-6\cb1 \uc0\u8232 \cb7 \uc0\u8232 \cb1 \uc0\u8232 \u8232 \u8232 \u8232 \u8232 \u8232 \cb11 Cloning into '/local/apps/OpenSoCo'...\cb1 \uc0\u8232 \cb11 Warning: Permanently added 'git.in.zhihu.com,10.159.0.14' (ECDSA) to the list of known hosts. \cb1 \uc0\u8232 \cb11 Switched to a new branch 'jeeves'\cb1 \uc0\u8232 \cb11 Branch jeeves set up to track remote branch jeeves from origin.\cb1 \uc0\u8232 \u8232 \cb11 now using node 10.204.1.191\cb1 \uc0\u8232 \cb11 Defaulting to user installation because normal site-packages is not writeable\cb1 \uc0\u8232 \cb11 Looking in indexes: https://mirror.in.zhihu.com/simple\cb1 \uc0\u8232 \cb11 TensorFlow installation not found - running with reduced feature set.\cb1 \uc0\u8232 \cb11 I0716 22:34:14.952005 139966050674432 plugin.py:429] Monitor runs begin\cb1 \uc0\u8232 \cb11 Collecting model_center==0.1.3\cb1 \uc0\u8232 \cb11 TensorBoard 2.10.0 at http://0.0.0.0:6006/klara-2-pek02/jeeves-agi/pytorchjob-wx-pretrain-scatch-390755/tensorboard/ (Press CTRL+C to quit)\cb1 \uc0\u8232 \cb11 Downloading https://pypi.in.zhihu.com/packages/70/74/ec3a3ce4a341102f4a473c6ca5798aef30c94943aadb8202132235aae919/model-center-0.1.3.tar.gz (46 kB)\cb1 \uc0\u8232 \cb11 \uc0\u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473 \u9473  46.9/46.9 kB 1.5 MB/s eta 0:00:00\cb1 \uc0\u8232 \cb11 Preparing metadata (setup.py): started\cb1 \uc0\u8232 \cb11 Preparing metadata (setup.py): finished with status 'done'\cb1 \uc0\u8232 \cb11 Requirement already satisfied: bmtrain in /home/jeeves/.local/lib/python3.7/site-packages (from model_center==0.1.3) (0.2.1)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: transformers in /home/jeeves/.local/lib/python3.7/site-packages (from model_center==0.1.3) (4.26.0.dev0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: jieba in /home/jeeves/.local/lib/python3.7/site-packages (from model_center==0.1.3) (0.42.1)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from bmtrain->model_center==0.1.3) (1.21.5)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (3.6.0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (6.0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: regex!=2019.12.17 in /home/jeeves/.local/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (2022.10.31)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (4.12.0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (21.3)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jeeves/.local/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (0.13.2)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (2.27.1)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/jeeves/.local/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (0.11.1)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers->model_center==0.1.3) (4.63.0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jeeves/.local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->model_center==0.1.3) (4.4.0)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers->model_center==0.1.3) (3.0.9)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->model_center==0.1.3) (3.8.1)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->model_center==0.1.3) (2022.6.15)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->model_center==0.1.3) (1.26.8)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->model_center==0.1.3) (3.3)\cb1 \uc0\u8232 \cb11 Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->model_center==0.1.3) (2.0.4)\cb1 \uc0\u8232 \cb11 Building wheels for collected packages: model_center\cb1 \uc0\u8232 \cb11 Building wheel for model_center (setup.py): started\cb1 \uc0\u8232 \cb11 Building wheel for model_center (setup.py): finished with status 'done'\cb1 \uc0\u8232 \cb11 Created wheel for model_center: filename=model_center-0.1.3-py3-none-any.whl size=93028 sha256=fd1a346dd3ef1e4e4ba1e8a29dffe0cef5a63547a6d2de29cfbd5cc0063523f3\cb1 \uc0\u8232 \cb11 Stored in directory: /home/jeeves/.cache/pip/wheels/17/47/79/64567dd03a90ef56258b093f45b708540a92aedc890f8ef4e0\cb1 \uc0\u8232 \cb11 Successfully built model_center\cb1 \uc0\u8232 \cb11 Installing collected packages: model_center\cb1 \uc0\u8232 \cb11 Successfully installed model_center-0.1.3\cb1 \uc0\u8232 \cb11 checkpoint-348500.pt\cb1 \uc0\u8232 \cb11 checkpoints\cb1 \uc0\u8232 \cb11 deberta-bmtrain.pt\cb1 \uc0\u8232 \cb11 init-deberta-bmtrain.pt\cb1 \uc0\u8232 \cb11 OpenSoCo_en\cb1 \uc0\u8232 \cb11 save\cb1 \uc0\u8232 \cb11 valid\cb1 \uc0\u8232 \cb11 70M /data/tensorboard\cb1 \uc0\u8232 \cb11 523G /data/checkpoints\cb1 \uc0\u8232 \cb11 4.0K /data/models\cb1 \uc0\u8232 \cb11 4.0K /data/logs\cb1 \uc0\u8232 \cb11 4.0K /data/results\cb1 \uc0\u8232 \cb11 4.0K /data/apps\cb1 \uc0\u8232 \cb11 523G /data\cb1 \uc0\u8232 \cb11 python3 -m torch.distributed.launch --nnodes=8 --nproc_per_node=8 --node_rank=0 --master_addr=pytorchjob-wx-pretrain-scatch-390755-master-0 --master_port=23456 ./train.py --vocab-file ./config/deberta_prenorm.json --model-config ./config/deberta_prenorm.json --input-dataset /mnt/data/user/tc_agi/user/wangxing/OpenSoCo_en/ --test-dataset /mnt/data/user/tc_agi/user/wangxing/valid/OpenSoCo_en/ --save /data/ --load /mnt/data/user/tc_agi/user/wangxing/init-deberta-bmtrain.pt --warmup-iters 10000 --lr-decay-style linear --lr-decay-iters 1000000 --weight-decay 0.01 --clip-grad 1.0 --loss-scale 524288 --start-step 0 --batch-size 16 --lr 7e-5 --save-iters 2500 --log-iters 100 --gradient-accumulate 1 --train-iters 1000000 --report_to tensorboard\cb1 \uc0\u8232 \cb11 /opt/conda/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\cb1 \uc0\u8232 \cb11 and will be removed in future. Use torchrun.\cb1 \uc0\u8232 \cb11 Note that --use_env is set by default in torchrun.\cb1 \uc0\u8232 \cb11 If your script expects `--local_rank` argument to be set, please\cb1 \uc0\u8232 \cb11 change it to read from `os.environ['LOCAL_RANK']` instead. See \cb1 \uc0\u8232 \cb11 https://pytorch.org/docs/stable/distributed.html#launch-utility for \cb1 \uc0\u8232 \cb11 further instructions\cb1 \uc0\u8232 \u8232 \cb11 FutureWarning,\cb1 \uc0\u8232 \cb11 WARNING:torch.distributed.run:\cb1 \uc0\u8232 \cb11 *****************************************\cb1 \uc0\u8232 \cb11 Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \cb1 \uc0\u8232 \cb11 *****************************************\cb1 \uc0\u8232 \cb11 Init bmp distributed.Init bmp distributed.\cb1 \uc0\u8232 \cb11 Init bmp distributed.Init bmp distributed.Init bmp distributed.Init bmp distributed.Init bmp distributed.\cb1 \uc0\u8232 \u8232 \u8232 \u8232 \u8232 \u8232 \cb11 Init bmp distributed.\cb1 \uc0\u8232 \cb11 NCCL version 2.10.3+cuda11.3\cb1 \uc0\u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 0\cb1 \uc0\u8232 \cb11 local_rank : 0\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 0\cb1 \uc0\u8232 \cb11 cpus : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1\cb1 \uc0\u8232 \cb11 3, 14, 15]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 1\cb1 \uc0\u8232 \cb11 local_rank : 1\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 1\cb1 \uc0\u8232 \cb11 cpus : [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\cb1 \uc0\u8232 \cb11 27, 28, 29, 30, 31]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 2\cb1 \uc0\u8232 \cb11 local_rank : 2\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 2\cb1 \uc0\u8232 \cb11 cpus : [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\cb1 \uc0\u8232 \cb11 43, 44, 45, 46, 47]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 3\cb1 \uc0\u8232 \cb11 local_rank : 3\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 3\cb1 \uc0\u8232 \cb11 cpus : [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,\cb1 \uc0\u8232 \cb11 59, 60, 61, 62, 63]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 4\cb1 \uc0\u8232 \cb11 local_rank : 4\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 4\cb1 \uc0\u8232 \cb11 cpus : [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\cb1 \uc0\u8232 \cb11 75, 76, 77, 78, 79]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 5\cb1 \uc0\u8232 \cb11 local_rank : 5\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 5\cb1 \uc0\u8232 \cb11 cpus : [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,\cb1 \uc0\u8232 \cb11 91, 92, 93, 94, 95]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 6\cb1 \uc0\u8232 \cb11 local_rank : 6\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 6\cb1 \uc0\u8232 \cb11 cpus : [96, 97, 98, 99, 100, 101, 102, 103, 104, 10\cb1 \uc0\u8232 \cb11 5, 106, 107, 108, 109, 110, 111]\cb1 \uc0\u8232 \u8232 \cb11 ====================== Initialization ======================\cb1 \uc0\u8232 \cb11 rank : 7\cb1 \uc0\u8232 \cb11 local_rank : 7\cb1 \uc0\u8232 \cb11 world_size : 64\cb1 \uc0\u8232 \cb11 local_size : 8\cb1 \uc0\u8232 \cb11 master : pytorchjob-wx-pretrain-scatch-390755-master-\cb1 \uc0\u8232 \cb11 0:23456\cb1 \uc0\u8232 \cb11 device : 7\cb1 \uc0\u8232 \cb11 cpus : [112, 113, 114, 115, 116, 117, 118, 119, 120\cb1 \uc0\u8232 \cb11 , 121, 122, 123, 124, 125, 126, 127]\cb1 \uc0\u8232 \u8232 \cb11 Namespace(base_path=None, batch_size=16, clip_grad=1.0, data_path=None, epochs=1, gradient_accumulate=1, input_dataset='/mnt/data/user/tc_agi/user/wangxing/OpenSoCo_en/', load='/mnt/data/user/tc_agi/user/wangxing/init-deberta-bmtrain.pt', load_path=None, local_rank=0, log_iters=100, loss_scale=524288.0, lr=7e-05, lr_decay_iters=1000000, lr_decay_style='linear', max_length=512, model_config='./config/deberta_prenorm.json', report_to='tensorboard', save='/data/', save_iters=2500, seed=42, start_step=207500, test_dataset='/mnt/data/user/tc_agi/user/wangxing/valid/OpenSoCo_en/', train_iters=1000000, valid_iters=500, vocab_file='./config/deberta_prenorm.json', warmup_iters=10000.0, weight_decay=0.01)\cb1 \uc0\u8232 \cb11 Loading from checkpoint-207500.pt...\cb1 \uc0\u8232 \cb11 Loading the optimizer...\cb1 \uc0\u8232 \cb11 param_groups [\{'lr': 7.603010101010102e-05, 'betas': (0.9, 0.98), 'eps': 1e-06, 'weight_decay': 0.01, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780]\}]\cb1 \uc0\u8232 \cb11 Model mem\cb1 \uc0\u8232 \cb11 |===========================================================================|\cb1 \uc0\u8232 \cb11 | PyTorch CUDA memory summary, device ID 0 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | CUDA OOMs: 0 | cudaMalloc retries: 0 |\cb1 \uc0\u8232 \cb11 |===========================================================================|\cb1 \uc0\u8232 \cb11 | Metric | Cur Usage | Peak Usage | Tot Alloc | Tot Freed |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Allocated memory | 46407 KB | 3089 MB | 8687 MB | 8641 MB |\cb1 \uc0\u8232 \cb11 | from large pool | 4713 KB | 3047 MB | 8576 MB | 8571 MB |\cb1 \uc0\u8232 \cb11 | from small pool | 41694 KB | 42 MB | 111 MB | 70 MB |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Active memory | 46407 KB | 3089 MB | 8687 MB | 8641 MB |\cb1 \uc0\u8232 \cb11 | from large pool | 4713 KB | 3047 MB | 8576 MB | 8571 MB |\cb1 \uc0\u8232 \cb11 | from small pool | 41694 KB | 42 MB | 111 MB | 70 MB |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | GPU reserved memory | 3130 MB | 3130 MB | 3130 MB | 0 B |\cb1 \uc0\u8232 \cb11 | from large pool | 3078 MB | 3078 MB | 3078 MB | 0 B |\cb1 \uc0\u8232 \cb11 | from small pool | 52 MB | 52 MB | 52 MB | 0 B |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Non-releasable memory | 23224 KB | 172917 KB | 4251 MB | 4228 MB |\cb1 \uc0\u8232 \cb11 | from large pool | 15767 KB | 167319 KB | 3964 MB | 3949 MB |\cb1 \uc0\u8232 \cb11 | from small pool | 7457 KB | 9505 KB | 286 MB | 279 MB |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Allocations | 62 | 845 | 6778 | 6716 |\cb1 \uc0\u8232 \cb11 | from large pool | 2 | 296 | 928 | 926 |\cb1 \uc0\u8232 \cb11 | from small pool | 60 | 550 | 5850 | 5790 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Active allocs | 62 | 845 | 6778 | 6716 |\cb1 \uc0\u8232 \cb11 | from large pool | 2 | 296 | 928 | 926 |\cb1 \uc0\u8232 \cb11 | from small pool | 60 | 550 | 5850 | 5790 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | GPU reserved segments | 168 | 168 | 168 | 0 |\cb1 \uc0\u8232 \cb11 | from large pool | 142 | 142 | 142 | 0 |\cb1 \uc0\u8232 \cb11 | from small pool | 26 | 26 | 26 | 0 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Non-releasable allocs | 26 | 29 | 2141 | 2115 |\cb1 \uc0\u8232 \cb11 | from large pool | 1 | 5 | 189 | 188 |\cb1 \uc0\u8232 \cb11 | from small pool | 25 | 26 | 1952 | 1927 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Oversize allocations | 0 | 0 | 0 | 0 |\cb1 \uc0\u8232 \cb11 |---------------------------------------------------------------------------|\cb1 \uc0\u8232 \cb11 | Oversize GPU segments | 0 | 0 | 0 | 0 |\cb1 \uc0\u8232 \cb11 |===========================================================================|\cb1 \uc0\u8232 \u8232 \cb11 load dataset from path /mnt/data/user/tc_agi/user/wangxing/OpenSoCo_en/3\cb1 \uc0\u8232 \cb11 64271\cb1 \uc0\u8232 \cb11 64 64\cb1 \uc0\u8232 \cb11 64\cb1 \uc0\u8232 \u8232 \cb11 60 5 6464\cb1 \uc0\u8232 \u8232 \cb11 64\cb1 \uc0\u8232 \cb11 4 64\cb1 \uc0\u8232 \cb11 2985225 ====================== 20896575 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 5970450 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 0 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 17911350 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 2985225 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 8955675 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 14926125 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 11940900 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 5970450 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 20896575 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 0 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 2985225 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 14926125 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 17911350 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 8955675 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 11940900 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 14926125 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 0 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 17911350 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 5970450 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 11940900 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 20896575 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 2985225 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 8955675 0\cb1 \uc0\u8232 \cb11 2985225 ====================== 14926125 0\cb1 \uc0\u8232 \cb11 5 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 5 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 0 0\cb1 \uc0\u8232 \cb11 0 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 0 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 17911350 0\cb1 \uc0\u8232 \cb11 6 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 6 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 5970450 0\cb1 \uc0\u8232 \cb11 2 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 2 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 11940900 0\cb1 \uc0\u8232 \cb11 4 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 4 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 2985225 0\cb1 \uc0\u8232 \cb11 1 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 1 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 20896575 0\cb1 \uc0\u8232 \cb11 7 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 7 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 2985225 ====================== 8955675 0\cb1 \uc0\u8232 \cb11 3 finish load train DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 3 finish load train bert_dataset\cb1 \uc0\u8232 \cb11 5 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 5 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 1 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 1 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 0 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 0 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 7 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 7 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 6 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 6 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 2 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 2 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 3 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 34 finish load val bert_dataset \cb1 \uc0\u8232 \cb11 finish load val DistributedMMapIndexedDataset\cb1 \uc0\u8232 \cb11 4 finish load val bert_dataset\cb1 \uc0\u8232 \cb11 finish loading dataset.\cb1 \uc0\u8232 \cb11 start init tensorboard\cb1 \uc0\u8232 \cb11 init tensorboard_dir: /data/tensorboard/207500/20230716223835\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 22:38:55 | Iter: 207500 | valid loss: 1.0588\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 524288.000000 to 262144.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 262144.000000 to 131072.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-16 22:41:16 | Iter: 207600 | loss: 1.0531, average_loss: 1.0583 | lr: 5.6028e-05, scale: 65536.0000, grad_norm: 0.9609 | average_time: 1.3299\cb1 \uc0\u8232 \cb11 2023-07-16 22:43:37 | Iter: 207700 | loss: 1.0604, average_loss: 1.0591 | lr: 5.6021e-05, scale: 65536.0000, grad_norm: 0.9893 | average_time: 1.3976\cb1 \uc0\u8232 \cb11 2023-07-16 22:45:59 | Iter: 207800 | loss: 1.0580, average_loss: 1.0558 | lr: 5.6014e-05, scale: 65536.0000, grad_norm: 1.0256 | average_time: 1.4306\cb1 \uc0\u8232 \cb11 2023-07-16 22:48:19 | Iter: 207900 | loss: 1.0647, average_loss: 1.0691 | lr: 5.6007e-05, scale: 65536.0000, grad_norm: 1.0054 | average_time: 1.3681\cb1 \uc0\u8232 \cb11 2023-07-16 22:50:39 | Iter: 208000 | loss: 1.0597, average_loss: 1.0541 | lr: 5.6000e-05, scale: 65536.0000, grad_norm: 1.0653 | average_time: 1.4132\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 22:50:57 | Iter: 208000 | valid loss: 1.0453\cb1 \uc0\u8232 \cb11 2023-07-16 22:53:16 | Iter: 208100 | loss: 1.0541, average_loss: 1.0530 | lr: 5.5993e-05, scale: 65536.0000, grad_norm: 0.9190 | average_time: 1.3099\cb1 \uc0\u8232 \cb11 2023-07-16 22:55:36 | Iter: 208200 | loss: 1.0533, average_loss: 1.0558 | lr: 5.5986e-05, scale: 65536.0000, grad_norm: 1.0406 | average_time: 1.3612\cb1 \uc0\u8232 \cb11 2023-07-16 22:57:57 | Iter: 208300 | loss: 1.0537, average_loss: 1.0642 | lr: 5.5979e-05, scale: 65536.0000, grad_norm: 1.0034 | average_time: 1.4033\cb1 \uc0\u8232 \cb11 2023-07-16 23:00:19 | Iter: 208400 | loss: 1.0522, average_loss: 1.0522 | lr: 5.5972e-05, scale: 65536.0000, grad_norm: 1.0393 | average_time: 1.4169\cb1 \uc0\u8232 \cb11 2023-07-16 23:02:40 | Iter: 208500 | loss: 1.0563, average_loss: 1.0565 | lr: 5.5965e-05, scale: 65536.0000, grad_norm: 0.9581 | average_time: 1.4152\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 23:02:58 | Iter: 208500 | valid loss: 1.0414\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-16 23:05:17 | Iter: 208600 | loss: 1.0518, average_loss: 1.0411 | lr: 5.5958e-05, scale: 65536.0000, grad_norm: 0.0913 | average_time: 1.3564\cb1 \uc0\u8232 \cb11 2023-07-16 23:07:37 | Iter: 208700 | loss: 1.0527, average_loss: 1.0595 | lr: 5.5951e-05, scale: 65536.0000, grad_norm: 1.0261 | average_time: 1.3984\cb1 \uc0\u8232 \cb11 2023-07-16 23:09:58 | Iter: 208800 | loss: 1.0535, average_loss: 1.0470 | lr: 5.5944e-05, scale: 65536.0000, grad_norm: 0.9578 | average_time: 1.3976\cb1 \uc0\u8232 \cb11 2023-07-16 23:12:19 | Iter: 208900 | loss: 1.0517, average_loss: 1.0465 | lr: 5.5937e-05, scale: 65536.0000, grad_norm: 1.0695 | average_time: 1.3480\cb1 \uc0\u8232 \cb11 2023-07-16 23:14:41 | Iter: 209000 | loss: 1.0504, average_loss: 1.0497 | lr: 5.5930e-05, scale: 65536.0000, grad_norm: 1.0450 | average_time: 1.4630\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 23:14:58 | Iter: 209000 | valid loss: 1.0456\cb1 \uc0\u8232 \cb11 2023-07-16 23:17:17 | Iter: 209100 | loss: 1.0502, average_loss: 1.0529 | lr: 5.5923e-05, scale: 65536.0000, grad_norm: 1.0023 | average_time: 1.3325\cb1 \uc0\u8232 \cb11 2023-07-16 23:19:37 | Iter: 209200 | loss: 1.0527, average_loss: 1.0530 | lr: 5.5915e-05, scale: 65536.0000, grad_norm: 1.0535 | average_time: 1.4484\cb1 \uc0\u8232 \cb11 2023-07-16 23:21:57 | Iter: 209300 | loss: 1.0515, average_loss: 1.0593 | lr: 5.5908e-05, scale: 65536.0000, grad_norm: 1.0109 | average_time: 1.4130\cb1 \uc0\u8232 \cb11 2023-07-16 23:24:17 | Iter: 209400 | loss: 1.0495, average_loss: 1.0473 | lr: 5.5901e-05, scale: 65536.0000, grad_norm: 1.0333 | average_time: 1.3723\cb1 \uc0\u8232 \cb11 2023-07-16 23:26:39 | Iter: 209500 | loss: 1.0500, average_loss: 1.0517 | lr: 5.5894e-05, scale: 65536.0000, grad_norm: 1.0222 | average_time: 1.4315\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 23:26:56 | Iter: 209500 | valid loss: 1.0434\cb1 \uc0\u8232 \cb11 2023-07-16 23:29:13 | Iter: 209600 | loss: 1.0503, average_loss: 1.0479 | lr: 5.5887e-05, scale: 65536.0000, grad_norm: 1.0312 | average_time: 1.3659\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-16 23:31:32 | Iter: 209700 | loss: 1.0484, average_loss: 1.0537 | lr: 5.5880e-05, scale: 65536.0000, grad_norm: 1.0350 | average_time: 1.4156\cb1 \uc0\u8232 \cb11 2023-07-16 23:33:50 | Iter: 209800 | loss: 1.0477, average_loss: 1.0462 | lr: 5.5873e-05, scale: 65536.0000, grad_norm: 1.0047 | average_time: 1.3496\cb1 \uc0\u8232 \cb11 2023-07-16 23:36:11 | Iter: 209900 | loss: 1.0487, average_loss: 1.0501 | lr: 5.5866e-05, scale: 65536.0000, grad_norm: 1.0332 | average_time: 1.4016\cb1 \uc0\u8232 \cb11 2023-07-16 23:38:29 | Iter: 210000 | loss: 1.0521, average_loss: 1.0559 | lr: 5.5859e-05, scale: 65536.0000, grad_norm: 1.0077 | average_time: 1.3730\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 23:38:49 | Iter: 210000 | valid loss: 1.0417\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-210000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 210000 step.\cb1 \uc0\u8232 \cb11 2023-07-16 23:41:21 | Iter: 210100 | loss: 1.0489, average_loss: 1.0437 | lr: 5.5852e-05, scale: 65536.0000, grad_norm: 1.0353 | average_time: 1.3269\cb1 \uc0\u8232 \cb11 2023-07-16 23:43:40 | Iter: 210200 | loss: 1.0535, average_loss: 1.0574 | lr: 5.5845e-05, scale: 65536.0000, grad_norm: 1.0149 | average_time: 1.4157\cb1 \uc0\u8232 \cb11 2023-07-16 23:45:59 | Iter: 210300 | loss: 1.0497, average_loss: 1.0476 | lr: 5.5838e-05, scale: 65536.0000, grad_norm: 0.9782 | average_time: 1.4058\cb1 \uc0\u8232 \cb11 2023-07-16 23:48:19 | Iter: 210400 | loss: 1.0526, average_loss: 1.0544 | lr: 5.5831e-05, scale: 65536.0000, grad_norm: 1.0095 | average_time: 1.3658\cb1 \uc0\u8232 \cb11 2023-07-16 23:50:39 | Iter: 210500 | loss: 1.0524, average_loss: 1.0456 | lr: 5.5824e-05, scale: 65536.0000, grad_norm: 1.0333 | average_time: 1.3559\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-16 23:50:59 | Iter: 210500 | valid loss: 1.0386\cb1 \uc0\u8232 \cb11 2023-07-16 23:53:19 | Iter: 210600 | loss: 1.0524, average_loss: 1.0522 | lr: 5.5817e-05, scale: 65536.0000, grad_norm: 1.0187 | average_time: 1.3605\cb1 \uc0\u8232 \cb11 2023-07-16 23:55:39 | Iter: 210700 | loss: 1.0492, average_loss: 1.0495 | lr: 5.5809e-05, scale: 65536.0000, grad_norm: 1.0115 | average_time: 1.4028\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-16 23:57:59 | Iter: 210800 | loss: 1.0469, average_loss: 1.0522 | lr: 5.5802e-05, scale: 65536.0000, grad_norm: 1.0248 | average_time: 1.3600\cb1 \uc0\u8232 \cb11 2023-07-17 00:00:20 | Iter: 210900 | loss: 1.0551, average_loss: 1.0582 | lr: 5.5795e-05, scale: 65536.0000, grad_norm: 1.0082 | average_time: 1.3835\cb1 \uc0\u8232 \cb11 2023-07-17 00:02:40 | Iter: 211000 | loss: 1.0501, average_loss: 1.0455 | lr: 5.5788e-05, scale: 65536.0000, grad_norm: 1.0451 | average_time: 1.4246\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 00:02:59 | Iter: 211000 | valid loss: 1.0340\cb1 \uc0\u8232 \cb11 2023-07-17 00:05:18 | Iter: 211100 | loss: 1.0486, average_loss: 1.0449 | lr: 5.5781e-05, scale: 65536.0000, grad_norm: 1.0055 | average_time: 1.3731\cb1 \uc0\u8232 \cb11 2023-07-17 00:07:39 | Iter: 211200 | loss: 1.0491, average_loss: 1.0465 | lr: 5.5774e-05, scale: 65536.0000, grad_norm: 1.0440 | average_time: 1.3752\cb1 \uc0\u8232 \cb11 2023-07-17 00:09:54 | Iter: 211300 | loss: 1.0494, average_loss: 1.0520 | lr: 5.5767e-05, scale: 65536.0000, grad_norm: 1.0110 | average_time: 1.3237\cb1 \uc0\u8232 \cb11 2023-07-17 00:12:11 | Iter: 211400 | loss: 1.0474, average_loss: 1.0490 | lr: 5.5760e-05, scale: 65536.0000, grad_norm: 0.9939 | average_time: 1.2996\cb1 \uc0\u8232 \cb11 2023-07-17 00:14:25 | Iter: 211500 | loss: 1.0473, average_loss: 1.0435 | lr: 5.5753e-05, scale: 65536.0000, grad_norm: 1.0466 | average_time: 1.3010\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 00:14:42 | Iter: 211500 | valid loss: 1.0313\cb1 \uc0\u8232 \cb11 2023-07-17 00:16:57 | Iter: 211600 | loss: 1.0508, average_loss: 1.0536 | lr: 5.5746e-05, scale: 65536.0000, grad_norm: 1.0078 | average_time: 1.3315\cb1 \uc0\u8232 \cb11 2023-07-17 00:19:12 | Iter: 211700 | loss: 1.0525, average_loss: 1.0474 | lr: 5.5739e-05, scale: 65536.0000, grad_norm: 1.0385 | average_time: 1.3238\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 00:21:28 | Iter: 211800 | loss: 1.0469, average_loss: 1.0457 | lr: 5.5732e-05, scale: 65536.0000, grad_norm: 1.0244 | average_time: 1.3244\cb1 \uc0\u8232 \cb11 2023-07-17 00:23:43 | Iter: 211900 | loss: 1.0503, average_loss: 1.0512 | lr: 5.5725e-05, scale: 65536.0000, grad_norm: 1.0641 | average_time: 1.4060\cb1 \uc0\u8232 \cb11 2023-07-17 00:25:57 | Iter: 212000 | loss: 1.0485, average_loss: 1.0552 | lr: 5.5718e-05, scale: 65536.0000, grad_norm: 1.0211 | average_time: 1.3066\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 00:26:14 | Iter: 212000 | valid loss: 1.0434\cb1 \uc0\u8232 \cb11 2023-07-17 00:28:28 | Iter: 212100 | loss: 1.0520, average_loss: 1.0524 | lr: 5.5711e-05, scale: 65536.0000, grad_norm: 1.0133 | average_time: 1.3024\cb1 \uc0\u8232 \cb11 2023-07-17 00:30:45 | Iter: 212200 | loss: 1.0541, average_loss: 1.0572 | lr: 5.5704e-05, scale: 65536.0000, grad_norm: 1.0177 | average_time: 1.3490\cb1 \uc0\u8232 \cb11 2023-07-17 00:33:00 | Iter: 212300 | loss: 1.0532, average_loss: 1.0519 | lr: 5.5696e-05, scale: 65536.0000, grad_norm: 1.0174 | average_time: 1.3181\cb1 \uc0\u8232 \cb11 2023-07-17 00:35:16 | Iter: 212400 | loss: 1.0567, average_loss: 1.0628 | lr: 5.5689e-05, scale: 65536.0000, grad_norm: 1.0130 | average_time: 1.3567\cb1 \uc0\u8232 \cb11 2023-07-17 00:37:32 | Iter: 212500 | loss: 1.0516, average_loss: 1.0480 | lr: 5.5682e-05, scale: 65536.0000, grad_norm: 1.0543 | average_time: 1.3531\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 00:37:49 | Iter: 212500 | valid loss: 1.0368\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-212500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 212500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 00:40:17 | Iter: 212600 | loss: 1.0475, average_loss: 1.0497 | lr: 5.5675e-05, scale: 65536.0000, grad_norm: 1.0249 | average_time: 1.3271\cb1 \uc0\u8232 \cb11 2023-07-17 00:42:31 | Iter: 212700 | loss: 1.0506, average_loss: 1.0541 | lr: 5.5668e-05, scale: 65536.0000, grad_norm: 1.0277 | average_time: 1.3189\cb1 \uc0\u8232 \cb11 2023-07-17 00:44:47 | Iter: 212800 | loss: 1.0510, average_loss: 1.0599 | lr: 5.5661e-05, scale: 131072.0000, grad_norm: 1.0066 | average_time: 1.3474\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 00:47:01 | Iter: 212900 | loss: 1.0455, average_loss: 1.0469 | lr: 5.5654e-05, scale: 65536.0000, grad_norm: 1.0107 | average_time: 1.3073\cb1 \uc0\u8232 \cb11 2023-07-17 00:49:15 | Iter: 213000 | loss: 1.0502, average_loss: 1.0536 | lr: 5.5647e-05, scale: 65536.0000, grad_norm: 1.0743 | average_time: 1.3624\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 00:49:32 | Iter: 213000 | valid loss: 1.0356\cb1 \uc0\u8232 \cb11 2023-07-17 00:51:51 | Iter: 213100 | loss: 1.0465, average_loss: 1.0481 | lr: 5.5640e-05, scale: 65536.0000, grad_norm: 0.9665 | average_time: 1.3580\cb1 \uc0\u8232 \cb11 2023-07-17 00:54:08 | Iter: 213200 | loss: 1.0421, average_loss: 1.0389 | lr: 5.5633e-05, scale: 65536.0000, grad_norm: 1.0746 | average_time: 1.3701\cb1 \uc0\u8232 \cb11 2023-07-17 00:56:23 | Iter: 213300 | loss: 1.0451, average_loss: 1.0501 | lr: 5.5626e-05, scale: 65536.0000, grad_norm: 1.0243 | average_time: 1.3139\cb1 \uc0\u8232 \cb11 2023-07-17 00:58:37 | Iter: 213400 | loss: 1.0455, average_loss: 1.0482 | lr: 5.5619e-05, scale: 65536.0000, grad_norm: 1.0192 | average_time: 1.3527\cb1 \uc0\u8232 \cb11 2023-07-17 01:00:56 | Iter: 213500 | loss: 1.0436, average_loss: 1.0352 | lr: 5.5612e-05, scale: 65536.0000, grad_norm: 1.0748 | average_time: 1.3326\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:01:13 | Iter: 213500 | valid loss: 1.0306\cb1 \uc0\u8232 \cb11 2023-07-17 01:03:27 | Iter: 213600 | loss: 1.0436, average_loss: 1.0496 | lr: 5.5605e-05, scale: 65536.0000, grad_norm: 1.0103 | average_time: 1.3278\cb1 \uc0\u8232 \cb11 2023-07-17 01:05:41 | Iter: 213700 | loss: 1.0440, average_loss: 1.0457 | lr: 5.5598e-05, scale: 65536.0000, grad_norm: 1.0208 | average_time: 1.3139\cb1 \uc0\u8232 \cb11 2023-07-17 01:07:58 | Iter: 213800 | loss: 1.0431, average_loss: 1.0456 | lr: 5.5590e-05, scale: 65536.0000, grad_norm: 1.0480 | average_time: 1.3929\cb1 \uc0\u8232 \cb11 2023-07-17 01:10:14 | Iter: 213900 | loss: 1.0451, average_loss: 1.0469 | lr: 5.5583e-05, scale: 131072.0000, grad_norm: 1.0150 | average_time: 1.3654\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 01:12:29 | Iter: 214000 | loss: 1.0431, average_loss: 1.0395 | lr: 5.5576e-05, scale: 65536.0000, grad_norm: 1.0304 | average_time: 1.2979\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:12:46 | Iter: 214000 | valid loss: 1.0269\cb1 \uc0\u8232 \cb11 2023-07-17 01:15:01 | Iter: 214100 | loss: 1.0395, average_loss: 1.0420 | lr: 5.5569e-05, scale: 65536.0000, grad_norm: 0.9992 | average_time: 1.3151\cb1 \uc0\u8232 \cb11 2023-07-17 01:17:15 | Iter: 214200 | loss: 1.0401, average_loss: 1.0371 | lr: 5.5562e-05, scale: 65536.0000, grad_norm: 1.0389 | average_time: 1.3105\cb1 \uc0\u8232 \cb11 2023-07-17 01:19:30 | Iter: 214300 | loss: 1.0432, average_loss: 1.0495 | lr: 5.5555e-05, scale: 65536.0000, grad_norm: 1.0266 | average_time: 1.3035\cb1 \uc0\u8232 \cb11 2023-07-17 01:21:48 | Iter: 214400 | loss: 1.0484, average_loss: 1.0483 | lr: 5.5548e-05, scale: 65536.0000, grad_norm: 1.0705 | average_time: 1.4065\cb1 \uc0\u8232 \cb11 2023-07-17 01:24:04 | Iter: 214500 | loss: 1.0426, average_loss: 1.0394 | lr: 5.5541e-05, scale: 65536.0000, grad_norm: 1.0203 | average_time: 1.3303\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:24:20 | Iter: 214500 | valid loss: 1.0379\cb1 \uc0\u8232 \cb11 2023-07-17 01:26:38 | Iter: 214600 | loss: 1.0454, average_loss: 1.0453 | lr: 5.5534e-05, scale: 65536.0000, grad_norm: 1.0386 | average_time: 1.4017\cb1 \uc0\u8232 \cb11 2023-07-17 01:28:55 | Iter: 214700 | loss: 1.0497, average_loss: 1.0509 | lr: 5.5527e-05, scale: 65536.0000, grad_norm: 1.0090 | average_time: 1.3734\cb1 \uc0\u8232 \cb11 2023-07-17 01:31:09 | Iter: 214800 | loss: 1.0444, average_loss: 1.0451 | lr: 5.5520e-05, scale: 65536.0000, grad_norm: 1.0376 | average_time: 1.3450\cb1 \uc0\u8232 \cb11 2023-07-17 01:33:23 | Iter: 214900 | loss: 1.0514, average_loss: 1.0491 | lr: 5.5513e-05, scale: 65536.0000, grad_norm: 1.0022 | average_time: 1.3286\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 01:35:38 | Iter: 215000 | loss: 1.0444, average_loss: 1.0304 | lr: 5.5506e-05, scale: 65536.0000, grad_norm: 0.8867 | average_time: 1.3189\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:35:54 | Iter: 215000 | valid loss: 1.0266\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-215000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 215000 step.\cb1 \uc0\u8232 \cb11 2023-07-17 01:38:22 | Iter: 215100 | loss: 1.0531, average_loss: 1.0519 | lr: 5.5499e-05, scale: 65536.0000, grad_norm: 1.0241 | average_time: 1.3014\cb1 \uc0\u8232 \cb11 2023-07-17 01:40:36 | Iter: 215200 | loss: 1.0440, average_loss: 1.0380 | lr: 5.5492e-05, scale: 65536.0000, grad_norm: 1.0192 | average_time: 1.3010\cb1 \uc0\u8232 \cb11 2023-07-17 01:42:51 | Iter: 215300 | loss: 1.0454, average_loss: 1.0433 | lr: 5.5485e-05, scale: 65536.0000, grad_norm: 0.9997 | average_time: 1.3198\cb1 \uc0\u8232 \cb11 2023-07-17 01:45:06 | Iter: 215400 | loss: 1.0457, average_loss: 1.0418 | lr: 5.5477e-05, scale: 65536.0000, grad_norm: 1.0622 | average_time: 1.3238\cb1 \uc0\u8232 \cb11 2023-07-17 01:47:21 | Iter: 215500 | loss: 1.0425, average_loss: 1.0399 | lr: 5.5470e-05, scale: 65536.0000, grad_norm: 1.0360 | average_time: 1.3193\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:47:38 | Iter: 215500 | valid loss: 1.0372\cb1 \uc0\u8232 \cb11 2023-07-17 01:49:53 | Iter: 215600 | loss: 1.0466, average_loss: 1.0457 | lr: 5.5463e-05, scale: 65536.0000, grad_norm: 1.0527 | average_time: 1.3550\cb1 \uc0\u8232 \cb11 2023-07-17 01:52:10 | Iter: 215700 | loss: 1.0413, average_loss: 1.0406 | lr: 5.5456e-05, scale: 65536.0000, grad_norm: 1.0283 | average_time: 1.3227\cb1 \uc0\u8232 \cb11 2023-07-17 01:54:25 | Iter: 215800 | loss: 1.0436, average_loss: 1.0443 | lr: 5.5449e-05, scale: 65536.0000, grad_norm: 1.0278 | average_time: 1.3156\cb1 \uc0\u8232 \cb11 2023-07-17 01:56:41 | Iter: 215900 | loss: 1.0406, average_loss: 1.0423 | lr: 5.5442e-05, scale: 65536.0000, grad_norm: 1.0317 | average_time: 1.4024\cb1 \uc0\u8232 \cb11 2023-07-17 01:58:57 | Iter: 216000 | loss: 1.0450, average_loss: 1.0410 | lr: 5.5435e-05, scale: 131072.0000, grad_norm: 0.8244 | average_time: 1.3587\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 01:59:14 | Iter: 216000 | valid loss: 1.0269\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 02:01:37 | Iter: 216100 | loss: 1.0398, average_loss: 1.0419 | lr: 5.5428e-05, scale: 65536.0000, grad_norm: 0.9897 | average_time: 1.3635\cb1 \uc0\u8232 \cb11 2023-07-17 02:03:53 | Iter: 216200 | loss: 1.0395, average_loss: 1.0390 | lr: 5.5421e-05, scale: 65536.0000, grad_norm: 1.0598 | average_time: 1.3767\cb1 \uc0\u8232 \cb11 2023-07-17 02:06:08 | Iter: 216300 | loss: 1.0389, average_loss: 1.0348 | lr: 5.5414e-05, scale: 65536.0000, grad_norm: 1.0414 | average_time: 1.3780\cb1 \uc0\u8232 \cb11 2023-07-17 02:08:21 | Iter: 216400 | loss: 1.0422, average_loss: 1.0432 | lr: 5.5407e-05, scale: 65536.0000, grad_norm: 0.9921 | average_time: 1.2961\cb1 \uc0\u8232 \cb11 2023-07-17 02:10:37 | Iter: 216500 | loss: 1.0409, average_loss: 1.0456 | lr: 5.5400e-05, scale: 65536.0000, grad_norm: 1.0519 | average_time: 1.3363\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 02:10:53 | Iter: 216500 | valid loss: 1.0321\cb1 \uc0\u8232 \cb11 2023-07-17 02:13:08 | Iter: 216600 | loss: 1.0428, average_loss: 1.0460 | lr: 5.5393e-05, scale: 65536.0000, grad_norm: 0.9995 | average_time: 1.3192\cb1 \uc0\u8232 \cb11 2023-07-17 02:15:22 | Iter: 216700 | loss: 1.0427, average_loss: 1.0445 | lr: 5.5386e-05, scale: 65536.0000, grad_norm: 1.0473 | average_time: 1.3040\cb1 \uc0\u8232 \cb11 2023-07-17 02:17:39 | Iter: 216800 | loss: 1.0449, average_loss: 1.0499 | lr: 5.5379e-05, scale: 65536.0000, grad_norm: 0.9965 | average_time: 1.3572\cb1 \uc0\u8232 \cb11 2023-07-17 02:19:53 | Iter: 216900 | loss: 1.0463, average_loss: 1.0392 | lr: 5.5371e-05, scale: 65536.0000, grad_norm: 1.0275 | average_time: 1.3195\cb1 \uc0\u8232 \cb11 2023-07-17 02:22:07 | Iter: 217000 | loss: 1.0361, average_loss: 1.0355 | lr: 5.5364e-05, scale: 65536.0000, grad_norm: 0.9963 | average_time: 1.3138\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 02:22:23 | Iter: 217000 | valid loss: 1.0216\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 02:24:40 | Iter: 217100 | loss: 1.0325, average_loss: 1.0278 | lr: 5.5357e-05, scale: 65536.0000, grad_norm: 0.7202 | average_time: 1.3994\cb1 \uc0\u8232 \cb11 2023-07-17 02:26:55 | Iter: 217200 | loss: 1.0392, average_loss: 1.0396 | lr: 5.5350e-05, scale: 65536.0000, grad_norm: 1.0193 | average_time: 1.3500\cb1 \uc0\u8232 \cb11 2023-07-17 02:29:10 | Iter: 217300 | loss: 1.0355, average_loss: 1.0302 | lr: 5.5343e-05, scale: 65536.0000, grad_norm: 0.9888 | average_time: 1.3624\cb1 \uc0\u8232 \cb11 2023-07-17 02:31:23 | Iter: 217400 | loss: 1.0432, average_loss: 1.0459 | lr: 5.5336e-05, scale: 65536.0000, grad_norm: 1.0308 | average_time: 1.3406\cb1 \uc0\u8232 \cb11 2023-07-17 02:33:38 | Iter: 217500 | loss: 1.0451, average_loss: 1.0371 | lr: 5.5329e-05, scale: 65536.0000, grad_norm: 0.9977 | average_time: 1.3833\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 02:33:55 | Iter: 217500 | valid loss: 1.0306\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-217500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 217500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 02:36:23 | Iter: 217600 | loss: 1.0444, average_loss: 1.0483 | lr: 5.5322e-05, scale: 65536.0000, grad_norm: 1.0383 | average_time: 1.3366\cb1 \uc0\u8232 \cb11 2023-07-17 02:38:39 | Iter: 217700 | loss: 1.0408, average_loss: 1.0415 | lr: 5.5315e-05, scale: 65536.0000, grad_norm: 0.9563 | average_time: 1.3542\cb1 \uc0\u8232 \cb11 2023-07-17 02:40:57 | Iter: 217800 | loss: 1.0396, average_loss: 1.0382 | lr: 5.5308e-05, scale: 65536.0000, grad_norm: 1.0608 | average_time: 1.3960\cb1 \uc0\u8232 \cb11 2023-07-17 02:43:13 | Iter: 217900 | loss: 1.0425, average_loss: 1.0434 | lr: 5.5301e-05, scale: 65536.0000, grad_norm: 1.0015 | average_time: 1.3184\cb1 \uc0\u8232 \cb11 2023-07-17 02:45:29 | Iter: 218000 | loss: 1.0445, average_loss: 1.0424 | lr: 5.5294e-05, scale: 65536.0000, grad_norm: 1.0422 | average_time: 1.3240\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 02:45:45 | Iter: 218000 | valid loss: 1.0276\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 02:48:01 | Iter: 218100 | loss: 1.0405, average_loss: 1.0378 | lr: 5.5287e-05, scale: 65536.0000, grad_norm: 0.8898 | average_time: 1.3185\cb1 \uc0\u8232 \cb11 2023-07-17 02:50:18 | Iter: 218200 | loss: 1.0419, average_loss: 1.0499 | lr: 5.5280e-05, scale: 65536.0000, grad_norm: 1.0188 | average_time: 1.3384\cb1 \uc0\u8232 \cb11 2023-07-17 02:52:34 | Iter: 218300 | loss: 1.0424, average_loss: 1.0369 | lr: 5.5273e-05, scale: 65536.0000, grad_norm: 1.0244 | average_time: 1.3740\cb1 \uc0\u8232 \cb11 2023-07-17 02:54:47 | Iter: 218400 | loss: 1.0432, average_loss: 1.0451 | lr: 5.5266e-05, scale: 65536.0000, grad_norm: 1.0160 | average_time: 1.3101\cb1 \uc0\u8232 \cb11 2023-07-17 02:57:03 | Iter: 218500 | loss: 1.0443, average_loss: 1.0369 | lr: 5.5258e-05, scale: 65536.0000, grad_norm: 1.0267 | average_time: 1.3203\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 02:57:19 | Iter: 218500 | valid loss: 1.0248\cb1 \uc0\u8232 \cb11 2023-07-17 02:59:32 | Iter: 218600 | loss: 1.0427, average_loss: 1.0473 | lr: 5.5251e-05, scale: 65536.0000, grad_norm: 1.0042 | average_time: 1.3565\cb1 \uc0\u8232 \cb11 2023-07-17 03:01:50 | Iter: 218700 | loss: 1.0431, average_loss: 1.0417 | lr: 5.5244e-05, scale: 65536.0000, grad_norm: 1.0172 | average_time: 1.4056\cb1 \uc0\u8232 \cb11 2023-07-17 03:04:05 | Iter: 218800 | loss: 1.0410, average_loss: 1.0417 | lr: 5.5237e-05, scale: 65536.0000, grad_norm: 0.9218 | average_time: 1.3219\cb1 \uc0\u8232 \cb11 2023-07-17 03:06:20 | Iter: 218900 | loss: 1.0400, average_loss: 1.0356 | lr: 5.5230e-05, scale: 65536.0000, grad_norm: 1.0787 | average_time: 1.3205\cb1 \uc0\u8232 \cb11 2023-07-17 03:08:34 | Iter: 219000 | loss: 1.0381, average_loss: 1.0412 | lr: 5.5223e-05, scale: 65536.0000, grad_norm: 1.0632 | average_time: 1.3598\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 03:08:52 | Iter: 219000 | valid loss: 1.0296\cb1 \uc0\u8232 \cb11 2023-07-17 03:11:07 | Iter: 219100 | loss: 1.0401, average_loss: 1.0403 | lr: 5.5216e-05, scale: 65536.0000, grad_norm: 1.0353 | average_time: 1.3712\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 03:13:22 | Iter: 219200 | loss: 1.0386, average_loss: 1.0460 | lr: 5.5209e-05, scale: 65536.0000, grad_norm: 1.0357 | average_time: 1.2933\cb1 \uc0\u8232 \cb11 2023-07-17 03:15:37 | Iter: 219300 | loss: 1.0432, average_loss: 1.0445 | lr: 5.5202e-05, scale: 65536.0000, grad_norm: 1.0129 | average_time: 1.3194\cb1 \uc0\u8232 \cb11 2023-07-17 03:17:55 | Iter: 219400 | loss: 1.0401, average_loss: 1.0343 | lr: 5.5195e-05, scale: 65536.0000, grad_norm: 1.0639 | average_time: 1.3243\cb1 \uc0\u8232 \cb11 2023-07-17 03:20:08 | Iter: 219500 | loss: 1.0392, average_loss: 1.0406 | lr: 5.5188e-05, scale: 65536.0000, grad_norm: 1.0002 | average_time: 1.3256\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 03:20:25 | Iter: 219500 | valid loss: 1.0254\cb1 \uc0\u8232 \cb11 2023-07-17 03:22:40 | Iter: 219600 | loss: 1.0402, average_loss: 1.0365 | lr: 5.5181e-05, scale: 65536.0000, grad_norm: 1.0355 | average_time: 1.3639\cb1 \uc0\u8232 \cb11 2023-07-17 03:24:55 | Iter: 219700 | loss: 1.0423, average_loss: 1.0412 | lr: 5.5174e-05, scale: 65536.0000, grad_norm: 1.0181 | average_time: 1.3377\cb1 \uc0\u8232 \cb11 2023-07-17 03:27:09 | Iter: 219800 | loss: 1.0440, average_loss: 1.0465 | lr: 5.5167e-05, scale: 65536.0000, grad_norm: 1.0295 | average_time: 1.3128\cb1 \uc0\u8232 \cb11 2023-07-17 03:29:24 | Iter: 219900 | loss: 1.0462, average_loss: 1.0382 | lr: 5.5160e-05, scale: 65536.0000, grad_norm: 1.0164 | average_time: 1.3321\cb1 \uc0\u8232 \cb11 2023-07-17 03:31:51 | Iter: 220000 | loss: 1.0415, average_loss: 1.0353 | lr: 5.5153e-05, scale: 65536.0000, grad_norm: 1.0183 | average_time: 1.3770\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 03:32:06 | Iter: 220000 | valid loss: 1.0213\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-220000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 220000 step.\cb1 \uc0\u8232 \cb11 2023-07-17 03:34:35 | Iter: 220100 | loss: 1.0376, average_loss: 1.0377 | lr: 5.5145e-05, scale: 65536.0000, grad_norm: 1.0592 | average_time: 1.3802\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 03:36:50 | Iter: 220200 | loss: 1.0381, average_loss: 1.0305 | lr: 5.5138e-05, scale: 65536.0000, grad_norm: 0.9973 | average_time: 1.3320\cb1 \uc0\u8232 \cb11 2023-07-17 03:39:04 | Iter: 220300 | loss: 1.0437, average_loss: 1.0502 | lr: 5.5131e-05, scale: 65536.0000, grad_norm: 1.0378 | average_time: 1.3214\cb1 \uc0\u8232 \cb11 2023-07-17 03:41:21 | Iter: 220400 | loss: 1.0477, average_loss: 1.0489 | lr: 5.5124e-05, scale: 65536.0000, grad_norm: 1.0040 | average_time: 1.3276\cb1 \uc0\u8232 \cb11 2023-07-17 03:43:37 | Iter: 220500 | loss: 1.0428, average_loss: 1.0376 | lr: 5.5117e-05, scale: 65536.0000, grad_norm: 1.0228 | average_time: 1.3783\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 03:43:55 | Iter: 220500 | valid loss: 1.0242\cb1 \uc0\u8232 \cb11 2023-07-17 03:46:11 | Iter: 220600 | loss: 1.0460, average_loss: 1.0424 | lr: 5.5110e-05, scale: 65536.0000, grad_norm: 1.0256 | average_time: 1.3952\cb1 \uc0\u8232 \cb11 2023-07-17 03:48:25 | Iter: 220700 | loss: 1.0475, average_loss: 1.0440 | lr: 5.5103e-05, scale: 65536.0000, grad_norm: 1.0284 | average_time: 1.2892\cb1 \uc0\u8232 \cb11 2023-07-17 03:50:42 | Iter: 220800 | loss: 1.0444, average_loss: 1.0423 | lr: 5.5096e-05, scale: 65536.0000, grad_norm: 1.0354 | average_time: 1.3721\cb1 \uc0\u8232 \cb11 2023-07-17 03:52:57 | Iter: 220900 | loss: 1.0426, average_loss: 1.0422 | lr: 5.5089e-05, scale: 65536.0000, grad_norm: 1.0257 | average_time: 1.3246\cb1 \uc0\u8232 \cb11 2023-07-17 03:55:12 | Iter: 221000 | loss: 1.0458, average_loss: 1.0445 | lr: 5.5082e-05, scale: 65536.0000, grad_norm: 1.0085 | average_time: 1.3579\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 03:55:27 | Iter: 221000 | valid loss: 1.0303\cb1 \uc0\u8232 \cb11 2023-07-17 03:57:44 | Iter: 221100 | loss: 1.0441, average_loss: 1.0403 | lr: 5.5075e-05, scale: 65536.0000, grad_norm: 1.0637 | average_time: 1.3311\cb1 \uc0\u8232 \cb11 2023-07-17 03:59:58 | Iter: 221200 | loss: 1.0371, average_loss: 1.0399 | lr: 5.5068e-05, scale: 131072.0000, grad_norm: 0.4996 | average_time: 1.3195\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 04:02:16 | Iter: 221300 | loss: 1.0318, average_loss: 1.0368 | lr: 5.5061e-05, scale: 65536.0000, grad_norm: 1.1160 | average_time: 1.3505\cb1 \uc0\u8232 \cb11 2023-07-17 04:04:31 | Iter: 221400 | loss: 1.0384, average_loss: 1.0429 | lr: 5.5054e-05, scale: 65536.0000, grad_norm: 1.0310 | average_time: 1.3102\cb1 \uc0\u8232 \cb11 2023-07-17 04:06:47 | Iter: 221500 | loss: 1.0435, average_loss: 1.0455 | lr: 5.5047e-05, scale: 65536.0000, grad_norm: 1.0121 | average_time: 1.4029\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 04:07:03 | Iter: 221500 | valid loss: 1.0276\cb1 \uc0\u8232 \cb11 2023-07-17 04:09:17 | Iter: 221600 | loss: 1.0430, average_loss: 1.0445 | lr: 5.5040e-05, scale: 65536.0000, grad_norm: 1.0283 | average_time: 1.3053\cb1 \uc0\u8232 \cb11 2023-07-17 04:11:33 | Iter: 221700 | loss: 1.0441, average_loss: 1.0409 | lr: 5.5032e-05, scale: 65536.0000, grad_norm: 0.9653 | average_time: 1.3770\cb1 \uc0\u8232 \cb11 2023-07-17 04:13:48 | Iter: 221800 | loss: 1.0421, average_loss: 1.0367 | lr: 5.5025e-05, scale: 65536.0000, grad_norm: 1.0662 | average_time: 1.3340\cb1 \uc0\u8232 \cb11 2023-07-17 04:16:01 | Iter: 221900 | loss: 1.0412, average_loss: 1.0384 | lr: 5.5018e-05, scale: 65536.0000, grad_norm: 0.9860 | average_time: 1.3472\cb1 \uc0\u8232 \cb11 2023-07-17 04:18:16 | Iter: 222000 | loss: 1.0455, average_loss: 1.0448 | lr: 5.5011e-05, scale: 65536.0000, grad_norm: 1.0137 | average_time: 1.3256\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 04:18:32 | Iter: 222000 | valid loss: 1.0281\cb1 \uc0\u8232 \cb11 2023-07-17 04:20:48 | Iter: 222100 | loss: 1.0451, average_loss: 1.0332 | lr: 5.5004e-05, scale: 65536.0000, grad_norm: 1.0418 | average_time: 1.3372\cb1 \uc0\u8232 \cb11 2023-07-17 04:23:04 | Iter: 222200 | loss: 1.0366, average_loss: 1.0384 | lr: 5.4997e-05, scale: 65536.0000, grad_norm: 0.9986 | average_time: 1.3410\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 04:25:19 | Iter: 222300 | loss: 1.0362, average_loss: 1.0263 | lr: 5.4990e-05, scale: 65536.0000, grad_norm: 0.8931 | average_time: 1.3074\cb1 \uc0\u8232 \cb11 2023-07-17 04:27:36 | Iter: 222400 | loss: 1.0438, average_loss: 1.0441 | lr: 5.4983e-05, scale: 65536.0000, grad_norm: 0.9788 | average_time: 1.4072\cb1 \uc0\u8232 \cb11 2023-07-17 04:29:52 | Iter: 222500 | loss: 1.0450, average_loss: 1.0481 | lr: 5.4976e-05, scale: 65536.0000, grad_norm: 1.0724 | average_time: 1.3423\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 04:30:10 | Iter: 222500 | valid loss: 1.0264\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-222500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 222500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 04:32:43 | Iter: 222600 | loss: 1.0416, average_loss: 1.0449 | lr: 5.4969e-05, scale: 65536.0000, grad_norm: 0.9960 | average_time: 1.3302\cb1 \uc0\u8232 \cb11 2023-07-17 04:35:00 | Iter: 222700 | loss: 1.0429, average_loss: 1.0388 | lr: 5.4962e-05, scale: 65536.0000, grad_norm: 1.0477 | average_time: 1.3106\cb1 \uc0\u8232 \cb11 2023-07-17 04:37:14 | Iter: 222800 | loss: 1.0389, average_loss: 1.0411 | lr: 5.4955e-05, scale: 65536.0000, grad_norm: 1.0407 | average_time: 1.3217\cb1 \uc0\u8232 \cb11 2023-07-17 04:39:29 | Iter: 222900 | loss: 1.0442, average_loss: 1.0530 | lr: 5.4948e-05, scale: 65536.0000, grad_norm: 1.0087 | average_time: 1.3075\cb1 \uc0\u8232 \cb11 2023-07-17 04:41:46 | Iter: 223000 | loss: 1.0497, average_loss: 1.0494 | lr: 5.4941e-05, scale: 65536.0000, grad_norm: 1.0132 | average_time: 1.3952\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 04:42:02 | Iter: 223000 | valid loss: 1.0283\cb1 \uc0\u8232 \cb11 2023-07-17 04:44:18 | Iter: 223100 | loss: 1.0495, average_loss: 1.0443 | lr: 5.4934e-05, scale: 65536.0000, grad_norm: 1.0518 | average_time: 1.3230\cb1 \uc0\u8232 \cb11 2023-07-17 04:46:33 | Iter: 223200 | loss: 1.0378, average_loss: 1.0382 | lr: 5.4926e-05, scale: 65536.0000, grad_norm: 0.9865 | average_time: 1.3514\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 04:48:48 | Iter: 223300 | loss: 1.0391, average_loss: 1.0329 | lr: 5.4919e-05, scale: 65536.0000, grad_norm: 0.4241 | average_time: 1.3427\cb1 \uc0\u8232 \cb11 2023-07-17 04:51:02 | Iter: 223400 | loss: 1.0346, average_loss: 1.0408 | lr: 5.4912e-05, scale: 65536.0000, grad_norm: 1.0405 | average_time: 1.3499\cb1 \uc0\u8232 \cb11 2023-07-17 04:53:16 | Iter: 223500 | loss: 1.0438, average_loss: 1.0536 | lr: 5.4905e-05, scale: 65536.0000, grad_norm: 1.0403 | average_time: 1.3175\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 04:53:34 | Iter: 223500 | valid loss: 1.0305\cb1 \uc0\u8232 \cb11 2023-07-17 04:55:48 | Iter: 223600 | loss: 1.0442, average_loss: 1.0460 | lr: 5.4898e-05, scale: 65536.0000, grad_norm: 1.0180 | average_time: 1.3361\cb1 \uc0\u8232 \cb11 2023-07-17 04:58:04 | Iter: 223700 | loss: 1.0413, average_loss: 1.0422 | lr: 5.4891e-05, scale: 65536.0000, grad_norm: 1.0407 | average_time: 1.3146\cb1 \uc0\u8232 \cb11 2023-07-17 05:00:17 | Iter: 223800 | loss: 1.0429, average_loss: 1.0471 | lr: 5.4884e-05, scale: 65536.0000, grad_norm: 1.0131 | average_time: 1.2877\cb1 \uc0\u8232 \cb11 2023-07-17 05:02:30 | Iter: 223900 | loss: 1.0504, average_loss: 1.0499 | lr: 5.4877e-05, scale: 65536.0000, grad_norm: 1.0156 | average_time: 1.2922\cb1 \uc0\u8232 \cb11 2023-07-17 05:04:47 | Iter: 224000 | loss: 1.0421, average_loss: 1.0392 | lr: 5.4870e-05, scale: 65536.0000, grad_norm: 0.9724 | average_time: 1.3571\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 05:05:03 | Iter: 224000 | valid loss: 1.0183\cb1 \uc0\u8232 \cb11 2023-07-17 05:07:17 | Iter: 224100 | loss: 1.0396, average_loss: 1.0369 | lr: 5.4863e-05, scale: 65536.0000, grad_norm: 1.0631 | average_time: 1.3147\cb1 \uc0\u8232 \cb11 2023-07-17 05:09:33 | Iter: 224200 | loss: 1.0427, average_loss: 1.0496 | lr: 5.4856e-05, scale: 65536.0000, grad_norm: 1.0029 | average_time: 1.3479\cb1 \uc0\u8232 \cb11 2023-07-17 05:11:49 | Iter: 224300 | loss: 1.0447, average_loss: 1.0410 | lr: 5.4849e-05, scale: 65536.0000, grad_norm: 1.0265 | average_time: 1.3664\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 05:14:02 | Iter: 224400 | loss: 1.0410, average_loss: 1.0426 | lr: 5.4842e-05, scale: 65536.0000, grad_norm: 0.8979 | average_time: 1.3117\cb1 \uc0\u8232 \cb11 2023-07-17 05:16:17 | Iter: 224500 | loss: 1.0530, average_loss: 1.0460 | lr: 5.4835e-05, scale: 65536.0000, grad_norm: 1.0135 | average_time: 1.3760\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 05:16:35 | Iter: 224500 | valid loss: 1.0206\cb1 \uc0\u8232 \cb11 2023-07-17 05:18:51 | Iter: 224600 | loss: 1.0495, average_loss: 1.0483 | lr: 5.4828e-05, scale: 65536.0000, grad_norm: 1.0299 | average_time: 1.3553\cb1 \uc0\u8232 \cb11 2023-07-17 05:21:07 | Iter: 224700 | loss: 1.0505, average_loss: 1.0554 | lr: 5.4821e-05, scale: 65536.0000, grad_norm: 1.0299 | average_time: 1.3430\cb1 \uc0\u8232 \cb11 2023-07-17 05:23:23 | Iter: 224800 | loss: 1.0429, average_loss: 1.0352 | lr: 5.4813e-05, scale: 65536.0000, grad_norm: 0.8720 | average_time: 1.3339\cb1 \uc0\u8232 \cb11 2023-07-17 05:25:40 | Iter: 224900 | loss: 1.0399, average_loss: 1.0381 | lr: 5.4806e-05, scale: 65536.0000, grad_norm: 1.0730 | average_time: 1.3351\cb1 \uc0\u8232 \cb11 2023-07-17 05:27:57 | Iter: 225000 | loss: 1.0428, average_loss: 1.0333 | lr: 5.4799e-05, scale: 65536.0000, grad_norm: 0.8486 | average_time: 1.3410\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 05:28:13 | Iter: 225000 | valid loss: 1.0147\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-225000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 225000 step.\cb1 \uc0\u8232 \cb11 2023-07-17 05:30:42 | Iter: 225100 | loss: 1.0386, average_loss: 1.0376 | lr: 5.4792e-05, scale: 65536.0000, grad_norm: 1.0654 | average_time: 1.3531\cb1 \uc0\u8232 \cb11 2023-07-17 05:32:59 | Iter: 225200 | loss: 1.0442, average_loss: 1.0430 | lr: 5.4785e-05, scale: 65536.0000, grad_norm: 1.0278 | average_time: 1.3254\cb1 \uc0\u8232 \cb11 2023-07-17 05:35:16 | Iter: 225300 | loss: 1.0414, average_loss: 1.0518 | lr: 5.4778e-05, scale: 65536.0000, grad_norm: 1.0777 | average_time: 1.3557\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 05:37:30 | Iter: 225400 | loss: 1.0435, average_loss: 1.0323 | lr: 5.4771e-05, scale: 65536.0000, grad_norm: 0.1990 | average_time: 1.3397\cb1 \uc0\u8232 \cb11 2023-07-17 05:39:46 | Iter: 225500 | loss: 1.0393, average_loss: 1.0458 | lr: 5.4764e-05, scale: 65536.0000, grad_norm: 1.0311 | average_time: 1.3461\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 05:40:03 | Iter: 225500 | valid loss: 1.0266\cb1 \uc0\u8232 \cb11 2023-07-17 05:42:17 | Iter: 225600 | loss: 1.0395, average_loss: 1.0390 | lr: 5.4757e-05, scale: 65536.0000, grad_norm: 1.0118 | average_time: 1.3101\cb1 \uc0\u8232 \cb11 2023-07-17 05:44:34 | Iter: 225700 | loss: 1.0421, average_loss: 1.0417 | lr: 5.4750e-05, scale: 65536.0000, grad_norm: 1.0756 | average_time: 1.3958\cb1 \uc0\u8232 \cb11 2023-07-17 05:46:52 | Iter: 225800 | loss: 1.0367, average_loss: 1.0352 | lr: 5.4743e-05, scale: 65536.0000, grad_norm: 1.0091 | average_time: 1.3697\cb1 \uc0\u8232 \cb11 2023-07-17 05:49:07 | Iter: 225900 | loss: 1.0459, average_loss: 1.0522 | lr: 5.4736e-05, scale: 65536.0000, grad_norm: 1.0321 | average_time: 1.3684\cb1 \uc0\u8232 \cb11 2023-07-17 05:51:20 | Iter: 226000 | loss: 1.0487, average_loss: 1.0505 | lr: 5.4729e-05, scale: 65536.0000, grad_norm: 1.0332 | average_time: 1.3213\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 05:51:39 | Iter: 226000 | valid loss: 1.0302\cb1 \uc0\u8232 \cb11 2023-07-17 05:53:53 | Iter: 226100 | loss: 1.0484, average_loss: 1.0478 | lr: 5.4722e-05, scale: 65536.0000, grad_norm: 0.9965 | average_time: 1.3688\cb1 \uc0\u8232 \cb11 2023-07-17 05:56:09 | Iter: 226200 | loss: 1.0465, average_loss: 1.0425 | lr: 5.4715e-05, scale: 65536.0000, grad_norm: 1.0387 | average_time: 1.3518\cb1 \uc0\u8232 \cb11 2023-07-17 05:58:25 | Iter: 226300 | loss: 1.0465, average_loss: 1.0455 | lr: 5.4707e-05, scale: 65536.0000, grad_norm: 1.0380 | average_time: 1.3149\cb1 \uc0\u8232 \cb11 2023-07-17 06:00:40 | Iter: 226400 | loss: 1.0457, average_loss: 1.0448 | lr: 5.4700e-05, scale: 65536.0000, grad_norm: 0.9722 | average_time: 1.3216\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 06:02:56 | Iter: 226500 | loss: 1.0405, average_loss: 1.0414 | lr: 5.4693e-05, scale: 65536.0000, grad_norm: 1.0107 | average_time: 1.3332\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 06:03:12 | Iter: 226500 | valid loss: 1.0207\cb1 \uc0\u8232 \cb11 2023-07-17 06:05:26 | Iter: 226600 | loss: 1.0440, average_loss: 1.0398 | lr: 5.4686e-05, scale: 65536.0000, grad_norm: 1.0328 | average_time: 1.3174\cb1 \uc0\u8232 \cb11 2023-07-17 06:07:44 | Iter: 226700 | loss: 1.0431, average_loss: 1.0425 | lr: 5.4679e-05, scale: 65536.0000, grad_norm: 1.0561 | average_time: 1.3919\cb1 \uc0\u8232 \cb11 2023-07-17 06:10:00 | Iter: 226800 | loss: 1.0419, average_loss: 1.0399 | lr: 5.4672e-05, scale: 65536.0000, grad_norm: 0.9871 | average_time: 1.3690\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 65536.000000 to 32768.000000\cb1 \uc0\u8232 \cb11 2023-07-17 06:12:15 | Iter: 226900 | loss: nan, average_loss: 1.0352 | lr: 5.4665e-05, scale: 32768.0000, grad_norm: 1.0935 | average_time: 1.3195\cb1 \uc0\u8232 \cb11 2023-07-17 06:14:30 | Iter: 227000 | loss: 1.0431, average_loss: 1.0455 | lr: 5.4658e-05, scale: 32768.0000, grad_norm: 0.9980 | average_time: 1.3446\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 06:14:47 | Iter: 227000 | valid loss: 1.0235\cb1 \uc0\u8232 \cb11 2023-07-17 06:17:02 | Iter: 227100 | loss: 1.0488, average_loss: 1.0541 | lr: 5.4651e-05, scale: 32768.0000, grad_norm: 1.0406 | average_time: 1.3664\cb1 \uc0\u8232 \cb11 2023-07-17 06:19:18 | Iter: 227200 | loss: 1.0460, average_loss: 1.0453 | lr: 5.4644e-05, scale: 32768.0000, grad_norm: 0.9882 | average_time: 1.3214\cb1 \uc0\u8232 \cb11 2023-07-17 06:21:34 | Iter: 227300 | loss: 1.0429, average_loss: 1.0398 | lr: 5.4637e-05, scale: 32768.0000, grad_norm: 1.0470 | average_time: 1.3735\cb1 \uc0\u8232 \cb11 2023-07-17 06:23:50 | Iter: 227400 | loss: 1.0468, average_loss: 1.0501 | lr: 5.4630e-05, scale: 32768.0000, grad_norm: 1.0050 | average_time: 1.3362\cb1 \uc0\u8232 \cb11 2023-07-17 06:26:04 | Iter: 227500 | loss: 1.0466, average_loss: 1.0429 | lr: 5.4623e-05, scale: 32768.0000, grad_norm: 1.0565 | average_time: 1.3407\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 06:26:20 | Iter: 227500 | valid loss: 1.0181\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-227500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 227500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 06:28:49 | Iter: 227600 | loss: 1.0432, average_loss: 1.0514 | lr: 5.4616e-05, scale: 32768.0000, grad_norm: 1.0468 | average_time: 1.3761\cb1 \uc0\u8232 \cb11 2023-07-17 06:31:06 | Iter: 227700 | loss: 1.0527, average_loss: 1.0425 | lr: 5.4609e-05, scale: 32768.0000, grad_norm: 0.9192 | average_time: 1.3729\cb1 \uc0\u8232 \cb11 2023-07-17 06:33:23 | Iter: 227800 | loss: 1.0474, average_loss: 1.0499 | lr: 5.4602e-05, scale: 32768.0000, grad_norm: 1.0715 | average_time: 1.3218\cb1 \uc0\u8232 \cb11 2023-07-17 06:35:38 | Iter: 227900 | loss: 1.0456, average_loss: 1.0430 | lr: 5.4594e-05, scale: 65536.0000, grad_norm: 1.0003 | average_time: 1.3103\cb1 \uc0\u8232 \cb11 2023-07-17 06:37:54 | Iter: 228000 | loss: 1.0469, average_loss: 1.0489 | lr: 5.4587e-05, scale: 65536.0000, grad_norm: 1.0455 | average_time: 1.3376\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 06:38:09 | Iter: 228000 | valid loss: 1.0210\cb1 \uc0\u8232 \cb11 2023-07-17 06:40:23 | Iter: 228100 | loss: 1.0434, average_loss: 1.0469 | lr: 5.4580e-05, scale: 65536.0000, grad_norm: 1.0174 | average_time: 1.3102\cb1 \uc0\u8232 \cb11 2023-07-17 06:42:40 | Iter: 228200 | loss: 1.0479, average_loss: 1.0381 | lr: 5.4573e-05, scale: 65536.0000, grad_norm: 1.0162 | average_time: 1.3658\cb1 \uc0\u8232 \cb11 2023-07-17 06:44:55 | Iter: 228300 | loss: 1.0499, average_loss: 1.0453 | lr: 5.4566e-05, scale: 65536.0000, grad_norm: 1.0144 | average_time: 1.3473\cb1 \uc0\u8232 \cb11 2023-07-17 06:47:11 | Iter: 228400 | loss: 1.0499, average_loss: 1.0480 | lr: 5.4559e-05, scale: 65536.0000, grad_norm: 1.0176 | average_time: 1.3193\cb1 \uc0\u8232 \cb11 2023-07-17 06:49:27 | Iter: 228500 | loss: 1.0485, average_loss: 1.0439 | lr: 5.4552e-05, scale: 65536.0000, grad_norm: 1.0854 | average_time: 1.3196\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 06:49:45 | Iter: 228500 | valid loss: 1.0223\cb1 \uc0\u8232 \cb11 2023-07-17 06:52:00 | Iter: 228600 | loss: 1.0486, average_loss: 1.0514 | lr: 5.4545e-05, scale: 65536.0000, grad_norm: 1.0236 | average_time: 1.3557\cb1 \uc0\u8232 \cb11 2023-07-17 06:54:15 | Iter: 228700 | loss: 1.0470, average_loss: 1.0461 | lr: 5.4538e-05, scale: 65536.0000, grad_norm: 1.0358 | average_time: 1.3074\cb1 \uc0\u8232 \cb11 2023-07-17 06:56:33 | Iter: 228800 | loss: 1.0488, average_loss: 1.0505 | lr: 5.4531e-05, scale: 65536.0000, grad_norm: 1.0116 | average_time: 1.3631\cb1 \uc0\u8232 \cb11 2023-07-17 06:58:49 | Iter: 228900 | loss: 1.0478, average_loss: 1.0532 | lr: 5.4524e-05, scale: 131072.0000, grad_norm: 0.7075 | average_time: 1.3388\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 07:01:03 | Iter: 229000 | loss: 1.0505, average_loss: 1.0533 | lr: 5.4517e-05, scale: 65536.0000, grad_norm: 1.0093 | average_time: 1.3562\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:01:19 | Iter: 229000 | valid loss: 1.0300\cb1 \uc0\u8232 \cb11 2023-07-17 07:03:34 | Iter: 229100 | loss: 1.0535, average_loss: 1.0519 | lr: 5.4510e-05, scale: 65536.0000, grad_norm: 1.0361 | average_time: 1.3296\cb1 \uc0\u8232 \cb11 2023-07-17 07:05:49 | Iter: 229200 | loss: 1.0469, average_loss: 1.0450 | lr: 5.4503e-05, scale: 65536.0000, grad_norm: 1.0246 | average_time: 1.3537\cb1 \uc0\u8232 \cb11 2023-07-17 07:08:04 | Iter: 229300 | loss: 1.0458, average_loss: 1.0432 | lr: 5.4496e-05, scale: 65536.0000, grad_norm: 1.0314 | average_time: 1.3230\cb1 \uc0\u8232 \cb11 2023-07-17 07:10:21 | Iter: 229400 | loss: 1.0458, average_loss: 1.0420 | lr: 5.4488e-05, scale: 65536.0000, grad_norm: 1.0544 | average_time: 1.3280\cb1 \uc0\u8232 \cb11 2023-07-17 07:12:37 | Iter: 229500 | loss: 1.0412, average_loss: 1.0417 | lr: 5.4481e-05, scale: 65536.0000, grad_norm: 1.0104 | average_time: 1.4013\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:12:53 | Iter: 229500 | valid loss: 1.0178\cb1 \uc0\u8232 \cb11 2023-07-17 07:15:07 | Iter: 229600 | loss: 1.0446, average_loss: 1.0359 | lr: 5.4474e-05, scale: 65536.0000, grad_norm: 0.9699 | average_time: 1.3256\cb1 \uc0\u8232 \cb11 2023-07-17 07:17:21 | Iter: 229700 | loss: 1.0517, average_loss: 1.0495 | lr: 5.4467e-05, scale: 65536.0000, grad_norm: 1.0209 | average_time: 1.3272\cb1 \uc0\u8232 \cb11 2023-07-17 07:19:39 | Iter: 229800 | loss: 1.0490, average_loss: 1.0422 | lr: 5.4460e-05, scale: 65536.0000, grad_norm: 0.9277 | average_time: 1.3502\cb1 \uc0\u8232 \cb11 2023-07-17 07:21:56 | Iter: 229900 | loss: 1.0491, average_loss: 1.0489 | lr: 5.4453e-05, scale: 65536.0000, grad_norm: 1.0446 | average_time: 1.3633\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 07:24:09 | Iter: 230000 | loss: 1.0463, average_loss: 1.0508 | lr: 5.4446e-05, scale: 65536.0000, grad_norm: 1.0071 | average_time: 1.3239\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:24:25 | Iter: 230000 | valid loss: 1.0194\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-230000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 230000 step.\cb1 \uc0\u8232 \cb11 2023-07-17 07:26:54 | Iter: 230100 | loss: 1.0502, average_loss: 1.0573 | lr: 5.4439e-05, scale: 65536.0000, grad_norm: 1.0239 | average_time: 1.3404\cb1 \uc0\u8232 \cb11 2023-07-17 07:29:10 | Iter: 230200 | loss: 1.0503, average_loss: 1.0429 | lr: 5.4432e-05, scale: 65536.0000, grad_norm: 1.0392 | average_time: 1.4166\cb1 \uc0\u8232 \cb11 2023-07-17 07:31:27 | Iter: 230300 | loss: 1.0453, average_loss: 1.0453 | lr: 5.4425e-05, scale: 65536.0000, grad_norm: 1.0427 | average_time: 1.3203\cb1 \uc0\u8232 \cb11 2023-07-17 07:33:43 | Iter: 230400 | loss: 1.0449, average_loss: 1.0523 | lr: 5.4418e-05, scale: 65536.0000, grad_norm: 0.9992 | average_time: 1.4238\cb1 \uc0\u8232 \cb11 2023-07-17 07:35:58 | Iter: 230500 | loss: 1.0431, average_loss: 1.0395 | lr: 5.4411e-05, scale: 65536.0000, grad_norm: 1.0367 | average_time: 1.3494\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:36:16 | Iter: 230500 | valid loss: 1.0097\cb1 \uc0\u8232 \cb11 2023-07-17 07:38:29 | Iter: 230600 | loss: 1.0455, average_loss: 1.0466 | lr: 5.4404e-05, scale: 65536.0000, grad_norm: 1.0386 | average_time: 1.3217\cb1 \uc0\u8232 \cb11 2023-07-17 07:40:46 | Iter: 230700 | loss: 1.0475, average_loss: 1.0533 | lr: 5.4397e-05, scale: 65536.0000, grad_norm: 1.0214 | average_time: 1.3529\cb1 \uc0\u8232 \cb11 2023-07-17 07:43:03 | Iter: 230800 | loss: 1.0508, average_loss: 1.0471 | lr: 5.4390e-05, scale: 65536.0000, grad_norm: 1.0484 | average_time: 1.3693\cb1 \uc0\u8232 \cb11 2023-07-17 07:45:19 | Iter: 230900 | loss: 1.0502, average_loss: 1.0541 | lr: 5.4383e-05, scale: 65536.0000, grad_norm: 1.0278 | average_time: 1.3503\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 07:47:35 | Iter: 231000 | loss: 1.0469, average_loss: 1.0397 | lr: 5.4376e-05, scale: 65536.0000, grad_norm: 0.8121 | average_time: 1.3463\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:47:51 | Iter: 231000 | valid loss: 1.0095\cb1 \uc0\u8232 \cb11 2023-07-17 07:50:06 | Iter: 231100 | loss: 1.0488, average_loss: 1.0552 | lr: 5.4368e-05, scale: 65536.0000, grad_norm: 1.0595 | average_time: 1.3696\cb1 \uc0\u8232 \cb11 2023-07-17 07:52:22 | Iter: 231200 | loss: 1.0503, average_loss: 1.0546 | lr: 5.4361e-05, scale: 65536.0000, grad_norm: 1.0271 | average_time: 1.2940\cb1 \uc0\u8232 \cb11 2023-07-17 07:54:39 | Iter: 231300 | loss: 1.0556, average_loss: 1.0488 | lr: 5.4354e-05, scale: 65536.0000, grad_norm: 1.0081 | average_time: 1.4173\cb1 \uc0\u8232 \cb11 2023-07-17 07:56:54 | Iter: 231400 | loss: 1.0497, average_loss: 1.0504 | lr: 5.4347e-05, scale: 65536.0000, grad_norm: 1.0527 | average_time: 1.3285\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 65536.000000 to 32768.000000\cb1 \uc0\u8232 \cb11 2023-07-17 07:59:08 | Iter: 231500 | loss: 1.0505, average_loss: 1.0463 | lr: 5.4340e-05, scale: 32768.0000, grad_norm: 0.5386 | average_time: 1.3123\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 07:59:23 | Iter: 231500 | valid loss: 1.0171\cb1 \uc0\u8232 \cb11 2023-07-17 08:01:40 | Iter: 231600 | loss: 1.0453, average_loss: 1.0495 | lr: 5.4333e-05, scale: 32768.0000, grad_norm: 1.0345 | average_time: 1.3829\cb1 \uc0\u8232 \cb11 2023-07-17 08:03:53 | Iter: 231700 | loss: 1.0538, average_loss: 1.0510 | lr: 5.4326e-05, scale: 32768.0000, grad_norm: 1.0622 | average_time: 1.3332\cb1 \uc0\u8232 \cb11 2023-07-17 08:06:10 | Iter: 231800 | loss: 1.0475, average_loss: 1.0448 | lr: 5.4319e-05, scale: 32768.0000, grad_norm: 1.0460 | average_time: 1.3374\cb1 \uc0\u8232 \cb11 2023-07-17 08:08:24 | Iter: 231900 | loss: 1.0489, average_loss: 1.0498 | lr: 5.4312e-05, scale: 32768.0000, grad_norm: 1.0016 | average_time: 1.3093\cb1 \uc0\u8232 \cb11 2023-07-17 08:10:40 | Iter: 232000 | loss: 1.0516, average_loss: 1.0454 | lr: 5.4305e-05, scale: 32768.0000, grad_norm: 1.0222 | average_time: 1.3361\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 08:10:57 | Iter: 232000 | valid loss: 1.0227\cb1 \uc0\u8232 \cb11 2023-07-17 08:13:13 | Iter: 232100 | loss: 1.0535, average_loss: 1.0402 | lr: 5.4298e-05, scale: 32768.0000, grad_norm: 1.0302 | average_time: 1.3283\cb1 \uc0\u8232 \cb11 2023-07-17 08:15:32 | Iter: 232200 | loss: 1.0510, average_loss: 1.0524 | lr: 5.4291e-05, scale: 32768.0000, grad_norm: 1.0134 | average_time: 1.3911\cb1 \uc0\u8232 \cb11 2023-07-17 08:17:49 | Iter: 232300 | loss: 1.0513, average_loss: 1.0454 | lr: 5.4284e-05, scale: 32768.0000, grad_norm: 1.0480 | average_time: 1.3582\cb1 \uc0\u8232 \cb11 2023-07-17 08:20:04 | Iter: 232400 | loss: 1.0515, average_loss: 1.0551 | lr: 5.4277e-05, scale: 32768.0000, grad_norm: 1.0197 | average_time: 1.3227\cb1 \uc0\u8232 \cb11 2023-07-17 08:22:16 | Iter: 232500 | loss: 1.0487, average_loss: 1.0466 | lr: 5.4270e-05, scale: 32768.0000, grad_norm: 1.0172 | average_time: 1.3169\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 08:22:33 | Iter: 232500 | valid loss: 1.0105\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-232500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 232500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 08:25:02 | Iter: 232600 | loss: 1.0443, average_loss: 1.0346 | lr: 5.4262e-05, scale: 65536.0000, grad_norm: 0.9384 | average_time: 1.3685\cb1 \uc0\u8232 \cb11 2023-07-17 08:27:19 | Iter: 232700 | loss: 1.0502, average_loss: 1.0488 | lr: 5.4255e-05, scale: 65536.0000, grad_norm: 1.0203 | average_time: 1.3263\cb1 \uc0\u8232 \cb11 2023-07-17 08:29:33 | Iter: 232800 | loss: 1.0467, average_loss: 1.0426 | lr: 5.4248e-05, scale: 65536.0000, grad_norm: 1.0239 | average_time: 1.3420\cb1 \uc0\u8232 \cb11 2023-07-17 08:31:48 | Iter: 232900 | loss: 1.0512, average_loss: 1.0539 | lr: 5.4241e-05, scale: 65536.0000, grad_norm: 1.0320 | average_time: 1.3711\cb1 \uc0\u8232 \cb11 2023-07-17 08:34:03 | Iter: 233000 | loss: 1.0509, average_loss: 1.0479 | lr: 5.4234e-05, scale: 65536.0000, grad_norm: 1.0456 | average_time: 1.3199\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 08:34:18 | Iter: 233000 | valid loss: 1.0142\cb1 \uc0\u8232 \cb11 2023-07-17 08:36:36 | Iter: 233100 | loss: 1.0482, average_loss: 1.0470 | lr: 5.4227e-05, scale: 65536.0000, grad_norm: 1.0645 | average_time: 1.3841\cb1 \uc0\u8232 \cb11 2023-07-17 08:38:51 | Iter: 233200 | loss: 1.0521, average_loss: 1.0483 | lr: 5.4220e-05, scale: 65536.0000, grad_norm: 1.0102 | average_time: 1.3638\cb1 \uc0\u8232 \cb11 2023-07-17 08:41:06 | Iter: 233300 | loss: 1.0510, average_loss: 1.0480 | lr: 5.4213e-05, scale: 65536.0000, grad_norm: 1.0473 | average_time: 1.3434\cb1 \uc0\u8232 \cb11 2023-07-17 08:43:21 | Iter: 233400 | loss: 1.0492, average_loss: 1.0463 | lr: 5.4206e-05, scale: 65536.0000, grad_norm: 1.0194 | average_time: 1.3077\cb1 \uc0\u8232 \cb11 2023-07-17 08:45:35 | Iter: 233500 | loss: 1.0529, average_loss: 1.0556 | lr: 5.4199e-05, scale: 65536.0000, grad_norm: 1.0303 | average_time: 1.3171\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 08:45:52 | Iter: 233500 | valid loss: 1.0189\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 08:48:06 | Iter: 233600 | loss: 1.0457, average_loss: 1.0449 | lr: 5.4192e-05, scale: 65536.0000, grad_norm: 0.8005 | average_time: 1.3012\cb1 \uc0\u8232 \cb11 2023-07-17 08:50:22 | Iter: 233700 | loss: 1.0522, average_loss: 1.0492 | lr: 5.4185e-05, scale: 65536.0000, grad_norm: 1.0092 | average_time: 1.3464\cb1 \uc0\u8232 \cb11 2023-07-17 08:52:38 | Iter: 233800 | loss: 1.0524, average_loss: 1.0588 | lr: 5.4178e-05, scale: 65536.0000, grad_norm: 1.0157 | average_time: 1.3764\cb1 \uc0\u8232 \cb11 2023-07-17 08:54:51 | Iter: 233900 | loss: 1.0529, average_loss: 1.0500 | lr: 5.4171e-05, scale: 65536.0000, grad_norm: 1.0631 | average_time: 1.3177\cb1 \uc0\u8232 \cb11 2023-07-17 08:57:06 | Iter: 234000 | loss: 1.0489, average_loss: 1.0501 | lr: 5.4164e-05, scale: 65536.0000, grad_norm: 1.0529 | average_time: 1.3530\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 08:57:22 | Iter: 234000 | valid loss: 1.0208\cb1 \uc0\u8232 \cb11 2023-07-17 08:59:37 | Iter: 234100 | loss: 1.0494, average_loss: 1.0525 | lr: 5.4156e-05, scale: 65536.0000, grad_norm: 1.0110 | average_time: 1.3830\cb1 \uc0\u8232 \cb11 2023-07-17 09:01:53 | Iter: 234200 | loss: 1.0496, average_loss: 1.0461 | lr: 5.4149e-05, scale: 65536.0000, grad_norm: 1.0395 | average_time: 1.3451\cb1 \uc0\u8232 \cb11 2023-07-17 09:04:08 | Iter: 234300 | loss: 1.0548, average_loss: 1.0553 | lr: 5.4142e-05, scale: 65536.0000, grad_norm: 1.0250 | average_time: 1.3508\cb1 \uc0\u8232 \cb11 2023-07-17 09:06:25 | Iter: 234400 | loss: 1.0563, average_loss: 1.0523 | lr: 5.4135e-05, scale: 65536.0000, grad_norm: 0.9773 | average_time: 1.3486\cb1 \uc0\u8232 \cb11 2023-07-17 09:08:39 | Iter: 234500 | loss: 1.0497, average_loss: 1.0527 | lr: 5.4128e-05, scale: 65536.0000, grad_norm: 1.0910 | average_time: 1.3525\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 09:08:57 | Iter: 234500 | valid loss: 1.0163\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 09:11:13 | Iter: 234600 | loss: 1.0475, average_loss: 1.0498 | lr: 5.4121e-05, scale: 65536.0000, grad_norm: 1.0820 | average_time: 1.3635\cb1 \uc0\u8232 \cb11 2023-07-17 09:13:30 | Iter: 234700 | loss: 1.0481, average_loss: 1.0577 | lr: 5.4114e-05, scale: 65536.0000, grad_norm: 1.0215 | average_time: 1.3605\cb1 \uc0\u8232 \cb11 2023-07-17 09:15:46 | Iter: 234800 | loss: 1.0493, average_loss: 1.0509 | lr: 5.4107e-05, scale: 65536.0000, grad_norm: 1.0583 | average_time: 1.3197\cb1 \uc0\u8232 \cb11 2023-07-17 09:18:01 | Iter: 234900 | loss: 1.0532, average_loss: 1.0608 | lr: 5.4100e-05, scale: 65536.0000, grad_norm: 1.0424 | average_time: 1.3003\cb1 \uc0\u8232 \cb11 2023-07-17 09:20:15 | Iter: 235000 | loss: 1.0526, average_loss: 1.0537 | lr: 5.4093e-05, scale: 65536.0000, grad_norm: 1.0116 | average_time: 1.3236\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 09:20:32 | Iter: 235000 | valid loss: 1.0179\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-235000.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 235000 step.\cb1 \uc0\u8232 \cb11 2023-07-17 09:22:59 | Iter: 235100 | loss: 1.0545, average_loss: 1.0588 | lr: 5.4086e-05, scale: 65536.0000, grad_norm: 1.0384 | average_time: 1.3119\cb1 \uc0\u8232 \cb11 2023-07-17 09:25:15 | Iter: 235200 | loss: 1.0479, average_loss: 1.0486 | lr: 5.4079e-05, scale: 65536.0000, grad_norm: 0.9756 | average_time: 1.3328\cb1 \uc0\u8232 \cb11 2023-07-17 09:27:29 | Iter: 235300 | loss: 1.0501, average_loss: 1.0485 | lr: 5.4072e-05, scale: 65536.0000, grad_norm: 1.1067 | average_time: 1.3151\cb1 \uc0\u8232 \cb11 2023-07-17 09:29:45 | Iter: 235400 | loss: 1.0478, average_loss: 1.0456 | lr: 5.4065e-05, scale: 65536.0000, grad_norm: 1.0165 | average_time: 1.3597\cb1 \uc0\u8232 \cb11 2023-07-17 09:32:00 | Iter: 235500 | loss: 1.0511, average_loss: 1.0534 | lr: 5.4058e-05, scale: 65536.0000, grad_norm: 1.0637 | average_time: 1.3339\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 09:32:16 | Iter: 235500 | valid loss: 1.0180\cb1 \uc0\u8232 \cb11 2023-07-17 09:34:29 | Iter: 235600 | loss: 1.0477, average_loss: 1.0498 | lr: 5.4050e-05, scale: 65536.0000, grad_norm: 0.9362 | average_time: 1.2902\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 131072.000000 to 65536.000000\cb1 \uc0\u8232 \cb11 2023-07-17 09:36:46 | Iter: 235700 | loss: 1.0530, average_loss: 1.0576 | lr: 5.4043e-05, scale: 65536.0000, grad_norm: 1.0272 | average_time: 1.3855\cb1 \uc0\u8232 \cb11 2023-07-17 09:39:01 | Iter: 235800 | loss: 1.0549, average_loss: 1.0556 | lr: 5.4036e-05, scale: 65536.0000, grad_norm: 1.0532 | average_time: 1.3658\cb1 \uc0\u8232 \cb11 2023-07-17 09:41:17 | Iter: 235900 | loss: 1.0506, average_loss: 1.0497 | lr: 5.4029e-05, scale: 65536.0000, grad_norm: 1.0148 | average_time: 1.3515\cb1 \uc0\u8232 \cb11 2023-07-17 09:43:30 | Iter: 236000 | loss: 1.0502, average_loss: 1.0449 | lr: 5.4022e-05, scale: 65536.0000, grad_norm: 1.0000 | average_time: 1.2912\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 09:43:47 | Iter: 236000 | valid loss: 1.0169\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 65536.000000 to 32768.000000\cb1 \uc0\u8232 \cb11 2023-07-17 09:46:01 | Iter: 236100 | loss: 1.0552, average_loss: 1.0573 | lr: 5.4015e-05, scale: 32768.0000, grad_norm: 1.0297 | average_time: 1.3527\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 32768.000000 to 16384.000000\cb1 \uc0\u8232 \cb11 2023-07-17 09:48:17 | Iter: 236200 | loss: nan, average_loss: 1.0601 | lr: 5.4008e-05, scale: 16384.0000, grad_norm: 0.1088 | average_time: 1.2966\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 16384.000000 to 8192.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 8192.000000 to 4096.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 4096.000000 to 2048.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 2048.000000 to 1024.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 1024.000000 to 512.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 512.000000 to 256.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 256.000000 to 128.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 128.000000 to 64.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 64.000000 to 32.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 32.000000 to 16.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 16.000000 to 8.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 8.000000 to 4.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 4.000000 to 2.000000\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 2.000000 to 1.000000\cb1 \uc0\u8232 \cb11 2023-07-17 09:50:28 | Iter: 236300 | loss: nan, average_loss: 1.0456 | lr: 5.4002e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2755\cb1 \uc0\u8232 \cb11 2023-07-17 09:52:41 | Iter: 236400 | loss: nan, average_loss: 1.0456 | lr: 5.3995e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3941\cb1 \uc0\u8232 \cb11 2023-07-17 09:54:51 | Iter: 236500 | loss: nan, average_loss: 1.0456 | lr: 5.3988e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2759\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 09:55:07 | Iter: 236500 | valid loss: nan\cb1 \uc0\u8232 \cb11 2023-07-17 09:57:17 | Iter: 236600 | loss: nan, average_loss: 1.0456 | lr: 5.3981e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3020\cb1 \uc0\u8232 \cb11 2023-07-17 09:59:28 | Iter: 236700 | loss: nan, average_loss: 1.0456 | lr: 5.3974e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2758\cb1 \uc0\u8232 \cb11 2023-07-17 10:01:39 | Iter: 236800 | loss: nan, average_loss: 1.0456 | lr: 5.3967e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3120\cb1 \uc0\u8232 \cb11 2023-07-17 10:03:48 | Iter: 236900 | loss: nan, average_loss: 1.0456 | lr: 5.3960e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3150\cb1 \uc0\u8232 \cb11 2023-07-17 10:05:58 | Iter: 237000 | loss: nan, average_loss: 1.0456 | lr: 5.3953e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2722\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 10:06:15 | Iter: 237000 | valid loss: nan\cb1 \uc0\u8232 \cb11 2023-07-17 10:08:25 | Iter: 237100 | loss: nan, average_loss: 1.0456 | lr: 5.3946e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2536\cb1 \uc0\u8232 \cb11 2023-07-17 10:10:36 | Iter: 237200 | loss: nan, average_loss: 1.0456 | lr: 5.3939e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2750\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 2.000000 to 1.000000\cb1 \uc0\u8232 \cb11 2023-07-17 10:12:46 | Iter: 237300 | loss: nan, average_loss: 1.0456 | lr: 5.3932e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2739\cb1 \uc0\u8232 \cb11 2023-07-17 10:14:56 | Iter: 237400 | loss: nan, average_loss: 1.0456 | lr: 5.3924e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2821\cb1 \uc0\u8232 \cb11 2023-07-17 10:17:06 | Iter: 237500 | loss: nan, average_loss: 1.0456 | lr: 5.3917e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3032\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 10:17:22 | Iter: 237500 | valid loss: nan\cb1 \uc0\u8232 \cb11 saving status into: /data/checkpoints/checkpoint-237500.success\cb1 \uc0\u8232 \cb11 Saving checkpoint at 237500 step.\cb1 \uc0\u8232 \cb11 2023-07-17 10:19:46 | Iter: 237600 | loss: nan, average_loss: 1.0456 | lr: 5.3910e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2960\cb1 \uc0\u8232 \cb11 2023-07-17 10:21:57 | Iter: 237700 | loss: nan, average_loss: 1.0456 | lr: 5.3903e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2810\cb1 \uc0\u8232 \cb11 2023-07-17 10:24:09 | Iter: 237800 | loss: nan, average_loss: 1.0456 | lr: 5.3896e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2738\cb1 \uc0\u8232 \cb11 2023-07-17 10:26:19 | Iter: 237900 | loss: nan, average_loss: 1.0456 | lr: 5.3889e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2836\cb1 \uc0\u8232 \cb11 2023-07-17 10:28:29 | Iter: 238000 | loss: nan, average_loss: 1.0456 | lr: 5.3882e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2454\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 10:28:47 | Iter: 238000 | valid loss: nan\cb1 \uc0\u8232 \cb11 2023-07-17 10:30:57 | Iter: 238100 | loss: nan, average_loss: 1.0456 | lr: 5.3875e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2977\cb1 \uc0\u8232 \cb11 2023-07-17 10:33:10 | Iter: 238200 | loss: nan, average_loss: 1.0456 | lr: 5.3868e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3423\cb1 \uc0\u8232 \cb11 2023-07-17 10:35:21 | Iter: 238300 | loss: nan, average_loss: 1.0456 | lr: 5.3861e-05, scale: 2.0000, grad_norm: nan | average_time: 1.2789\cb1 \uc0\u8232 \cb11 Gradient overflow, change scale from 2.000000 to 1.000000\cb1 \uc0\u8232 \cb11 2023-07-17 10:37:34 | Iter: 238400 | loss: nan, average_loss: 1.0456 | lr: 5.3854e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3455\cb1 \uc0\u8232 \cb11 2023-07-17 10:39:44 | Iter: 238500 | loss: nan, average_loss: 1.0456 | lr: 5.3847e-05, scale: 1.0000, grad_norm: nan | average_time: 1.3598\cb1 \uc0\u8232 \cb11 start valid! \cb1 \uc0\u8232 \cb11 2023-07-17 10:39:59 | Iter: 238500 | valid loss: nan\cb1 \uc0\u8232 \cb11 2023-07-17 10:42:11 | Iter: 238600 | loss: nan, average_loss: 1.0456 | lr: 5.3840e-05, scale: 1.0000, grad_norm: nan | average_time: 1.2570}